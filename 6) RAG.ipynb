{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b014bff",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c26da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/gw8dmgsd6m11bg8nhrgpd3vr0000gn/T/ipykernel_73279/1292714131.py:15: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources            # package and dependency management\n"
     ]
    }
   ],
   "source": [
    "########################## UTILITY AND SYSTEM ##########################\n",
    "\n",
    "import os                       # filesystem operations\n",
    "import csv                      # reading/writing CSV files\n",
    "import json                     # JSON parsing and serialization\n",
    "import math                     # basic math functions\n",
    "import random                   # random number generation\n",
    "import time                     # time-related functions\n",
    "import tempfile                 # temporary file management\n",
    "import tarfile                  # tar archive handling\n",
    "import io                       # input/output streams\n",
    "import pickle                   # object serialization\n",
    "import importlib                # dynamic import of modules\n",
    "import multiprocessing          # parallel process management\n",
    "import pkg_resources            # package and dependency management\n",
    "from copy import deepcopy       # deep copy of objects\n",
    "from pathlib import Path        # filesystem paths handling (cross-platform)\n",
    "\n",
    "########################## DOWNLOAD ##########################\n",
    "\n",
    "import requests                 # HTTP requests library\n",
    "import wget                     # file downloads from URLs\n",
    "from urllib.request import urlopen  # open URLs (alternative to requests)\n",
    "\n",
    "########################## VISUALIZATION ##########################\n",
    "\n",
    "import matplotlib.pyplot as plt # basic plotting library\n",
    "import plotly.graph_objs as go  # interactive plotting\n",
    "from tqdm.notebook import tqdm  # progress bars for loops in notebooks\n",
    "from pprint import pprint       # formatted pretty-printing of objects\n",
    "\n",
    "########################## DATAFRAME ##########################\n",
    "\n",
    "import numpy as np              # numerical arrays and operations\n",
    "import pandas as pd             # dataframes and data manipulation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "########################## TEXT PROCESSING ##########################\n",
    "\n",
    "import re                      # regular expressions\n",
    "import string                  # string constants and operations\n",
    "from itertools import chain, islice  # advanced iteration and chaining\n",
    "\n",
    "########################## TOKENIZATION ##########################\n",
    "\n",
    "from collections import Counter, OrderedDict  # frequency counts and ordered dictionaries\n",
    "import nltk                                   # natural language processing toolkit\n",
    "from nltk.tokenize import word_tokenize       # word tokenization\n",
    "import spacy                                  # advanced NLP (tokenization, parsing)\n",
    "from torchtext.data.utils import get_tokenizer       # torchtext tokenizers\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator # build vocabulary from iterator\n",
    "\n",
    "########################## DATASET AND DATALOADER ##########################\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split   # datasets and data loading utilities\n",
    "from torch.nn.utils.rnn import pad_sequence                      # padding variable-length sequences\n",
    "from datasets import load_dataset, DatasetDict                   # HuggingFace datasets loading\n",
    "from torchtext.datasets import AG_NEWS                           # torchtext built-in datasets\n",
    "\n",
    "########################## PYTORCH AND DEEP LEARNING ##########################\n",
    "\n",
    "import torch                             # PyTorch main library\n",
    "from torch import nn, Tensor             # neural network modules and tensors\n",
    "from torch.nn import CrossEntropyLoss    # common loss function for classification\n",
    "\n",
    "########################## WORD EMBEDDING ##########################\n",
    "\n",
    "from torchtext.vocab import GloVe        # pretrained GloVe embeddings\n",
    "# from gensim.models import Word2Vec     # word2vec embeddings from corpus (commented out)\n",
    "\n",
    "########################## HUGGING FACE ##########################\n",
    "\n",
    "import transformers                      # transformers library core\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,     # GPT-2 tokenizer and model\n",
    "    BertTokenizer, BertTokenizerFast, BertConfig, BertForMaskedLM,  # BERT components\n",
    "    XLNetTokenizer,                     # XLNet tokenizer\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline,                          # easy pipelines for inference\n",
    "    AutoTokenizer,                    # auto tokenizer loader\n",
    "    AutoModelForCausalLM, GPT2ForSequenceClassification,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer,  # training utilities\n",
    "    set_seed, GenerationConfig,\n",
    "    BertModel                        # BERT base model\n",
    ")\n",
    "from datasets import DatasetDict         # HuggingFace dataset dictionaries\n",
    "\n",
    "######################### TRL & PEFT (TRAINING & PARAMETER EFFICIENT FINE-TUNING) ##########################\n",
    "\n",
    "from trl import (\n",
    "    SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM,\n",
    "    DPOConfig, DPOTrainer,\n",
    "    RewardTrainer, RewardConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torchmetrics import Accuracy        # metrics for evaluation\n",
    "\n",
    "########################## RAG ##########################\n",
    "\n",
    "from transformers import (\n",
    "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n",
    "    DPRContextEncoder, DPRContextEncoderTokenizer\n",
    ")\n",
    "import faiss                              # similarity search library\n",
    "\n",
    "########################## EVALUATION ##########################\n",
    "\n",
    "import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b0a3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which device we are on: cpu\n"
     ]
    }
   ],
   "source": [
    "def accelerator(where = \"mps\"):\n",
    "    if where == \"mps\":\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "    if where == \"cuda\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "    if where == \"cpu\":\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "\n",
    "device = accelerator(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c90fd",
   "metadata": {},
   "source": [
    "<span style=\"background-color: yellow\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697f770",
   "metadata": {},
   "source": [
    "# 0) CONCEPTS: RAG, FAISS, Prompt Engineering, LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c463b4",
   "metadata": {},
   "source": [
    "# 1) RAG with HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42610efd",
   "metadata": {},
   "source": [
    "The relevant context usually is in the form of type_of_file-content (es code_of_conduct-content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed0078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Treatment and Assistance: Employees with substance abuse issues are encouraged to seek help. The organization is committed to providing support, resources, and information to assist those seeking treatment.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'companyPolicies.txt'\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'\n",
    "\n",
    "# Use wget to download the file\n",
    "wget.download(url, out=filename)\n",
    "print('file downloaded')\n",
    "\n",
    "def read_and_split_text(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    # Split the text into paragraphs (simple split by newline characters)\n",
    "    paragraphs = text.split('\\n')\n",
    "    # Filter out any empty paragraphs or undesired entries\n",
    "    paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 0]\n",
    "    return paragraphs\n",
    "\n",
    "# Read the text file and split it into paragraphs\n",
    "\n",
    "paragraphs = read_and_split_text('companyPolicies.txt')\n",
    "random.shuffle(paragraphs) #shuffling samples so that the samples are not ordered based on the category they belong to\n",
    "paragraphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8111d2",
   "metadata": {},
   "source": [
    "## Build the context encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cee581",
   "metadata": {},
   "source": [
    "Let's use the Dense Passage Retriever (DPR) model, specifically the context encoder, to convert your preprocessed text data into dense vector embeddings. These embeddings capture the semantic meanings of the texts, enabling effective similarity-based retrieval. DPR models, such as the the DPRContextEncoder and DPRContextEncoderTokenizer, are built on the BERT architecture but specialize in dense passage retrieval. They differ from BERT in their training, which focuses on contrastive learning for retrieving relevant passages, while BERT is more general-purpose, handling various NLP tasks. Passages are:\n",
    "1. tokenize\n",
    "2. encode\n",
    "3. aggregate in a single vector\n",
    "\n",
    "At the end of this section there is a function which does everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e0fc7",
   "metadata": {},
   "source": [
    "Ignore the warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef2ba724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92056502f298429ba06bbbb252f4156c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d5102aa9c24185aabb1c8f94dd056d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731c5434ca70444693dd907507654797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4601e68569ec496bafc61ffcbecb806d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/492 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08de00949342443c86889ddf4aa00735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base', use_safetensors = True)\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base', use_safetensors = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272edafa",
   "metadata": {},
   "source": [
    "Usage Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47fdd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2129, 2024, 2017, 1029,  102, 1045, 2572, 2986, 1012,  102],\n",
      "        [ 101, 2054, 1005, 1055, 2039, 1029,  102, 2025, 2172, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} \n",
      "\n",
      "['[CLS]', 'how', 'are', 'you', '?', '[SEP]', 'i', 'am', 'fine', '.', '[SEP]']\n",
      "['[CLS]', 'what', \"'\", 's', 'up', '?', '[SEP]', 'not', 'much', '.', '[SEP]']\n",
      "tensor([[ 0.1901,  0.6006, -0.1140,  ..., -0.3477,  0.6554,  0.0928],\n",
      "        [ 0.6606,  0.3294,  0.3890,  ..., -0.0723,  0.3644, -0.1266]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "number of samples:\n",
      "1\n",
      " samples shape:\n",
      "torch.Size([2, 768])\n",
      "number of samples:\n",
      "2\n",
      " samples shape:\n",
      "torch.Size([2, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [(\"How are you?\", \"I am fine.\"), (\"What's up?\", \"Not much.\")]\n",
    "\n",
    "tokens_info = context_tokenizer(text, return_tensors = 'pt', padding = True, truncation = True, max_length = 256)\n",
    "\n",
    "print(tokens_info,'\\n')\n",
    "\n",
    "for s in tokens_info['input_ids']: #get the original text\n",
    "   print(context_tokenizer.convert_ids_to_tokens(s))\n",
    "\n",
    "context_encoder(**tokens_info) #embedding\n",
    "print(context_encoder(**tokens_info).pooler_output,'\\n') #embedding PyTorch tensor without other informations\n",
    "\n",
    "embeddings=[]\n",
    "for text in text:\n",
    "    inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "    outputs = context_encoder(**inputs)\n",
    "    embeddings.append(outputs.pooler_output)\n",
    "    print(\"number of samples:\")\n",
    "    print(len(embeddings))\n",
    "    print(\" samples shape:\")\n",
    "    print(outputs.pooler_output.shape) #the shape is [2,768]: 2 is len(text[i]) while 768 is the embedding dimension\n",
    "\n",
    "torch.cat(embeddings).detach().numpy().shape #this is the aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa861c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_contexts(text_list):\n",
    "    embeddings = []\n",
    "    for text in text_list:\n",
    "        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256) #tokenized input\n",
    "        outputs = context_encoder(**inputs)\n",
    "        embeddings.append(outputs.pooler_output) #embedding vectors in\n",
    "    return torch.cat(embeddings).detach().numpy() #aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9ad13",
   "metadata": {},
   "source": [
    "Example of usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9568f375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70083404,  0.5377584 ,  0.5114269 , ..., -0.32141268,\n",
       "         0.4081831 , -0.07802014],\n",
       "       [ 0.53100616, -0.2688355 , -0.08287751, ...,  0.24389295,\n",
       "         0.11508092,  0.06219082]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_contexts(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc699538",
   "metadata": {},
   "source": [
    "So for the paragraphs in our original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f0403df",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings = encode_contexts(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0c4b2",
   "metadata": {},
   "source": [
    "## FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95a5d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 768  # This should match the dimension of your embeddings\n",
    "context_embeddings_np = np.array(context_embeddings).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(context_embeddings_np)  # Add the context embeddings to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e17f1",
   "metadata": {},
   "source": [
    "## Build the question encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf66a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0662fc09613449dfae2ffe7609ca9a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/493 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b824a5b173954567af6fc7c2543aa540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c22db517c0e4b228f8b833c8db4325c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ab5ef53aea43738bce1b8191e5fa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c1e5e2ff2449ff9ff52b5bdd331406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base', use_safetensors = True)\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base', use_safetensors = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b825ad7",
   "metadata": {},
   "source": [
    "## Retrival process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a0a01",
   "metadata": {},
   "source": [
    "Now we have all the encoders and the distance metric (faiss), so now we can retrive. First, process an example query by converting the raw text question into a format that the DPR question encoder can understand and then encode it into a dense vector. Using the encoded question, search your prebuilt FAISS index to find the most relevant contexts. This step showcases the practical use of the FAISS index in retrieving information based on query similarity.\n",
    "\n",
    "After conducting the search for relevant contexts based on the question embedding, the output consists of two key components:\n",
    "\n",
    "- **D (Distances)**: This array contains the distances between the query embedding and the retrieved document embeddings. The distances measure the similarity between the query and each document, where lower distances indicate higher relevance. These values help determine how closely each retrieved context matches the query.\n",
    "\n",
    "- **I (Indices)**: This array holds the indices of the paragraphs within the `paragraphs` array that have been identified as the most relevant to the query. These indices correspond to the positions of the paragraphs in the original data array, allowing for easy retrieval of the actual text content.\n",
    "\n",
    "The combination of `D` and `I` provides both a quantitative measure of relevance and the specific content that is most relevant, enabling a comprehensive response to the user's query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26be6e",
   "metadata": {},
   "source": [
    "**Source of confusion**: even if in the following cell there is no direct mention to the context, it is already encapsulated in the faiss, which is called with `index.search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aeb5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[72.76533 74.71622 84.3881  88.36439 90.28713]]\n",
      "I: [[ 3 33 55 29 40]]\n"
     ]
    }
   ],
   "source": [
    "question = 'Drug and Alcohol Policy'\n",
    "question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
    "\n",
    "# Search the index\n",
    "D, I = index.search(question_embedding, k=5)  # Retrieve top 5 relevant contexts\n",
    "print(\"D:\",D)\n",
    "print(\"I:\",I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9be0a691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 relevant contexts:\n",
      "1: 6.\tDrug and Alcohol Policy\n",
      "distance 72.76532745361328\n",
      "\n",
      "2: Policy Objective: The Drug and Alcohol Policy is established to establish clear expectations and guidelines for the responsible use of drugs and alcohol within the organization. This policy aims to maintain a safe, healthy, and productive workplace.\n",
      "distance 74.71621704101562\n",
      "\n",
      "3: Testing and Searches: The organization reserves the right to conduct drug and alcohol testing as per applicable laws and regulations. Employees may be subject to testing in cases of reasonable suspicion, post-accident, or as part of routine workplace safety measures.\n",
      "distance 84.38809967041016\n",
      "\n",
      "4: 9.\tDiscipline and Termination Policy\n",
      "distance 88.36438751220703\n",
      "\n",
      "5: Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes.\n",
      "distance 90.2871322631836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 relevant contexts:\")\n",
    "for i, idx in enumerate(I[0]):\n",
    "    print(f\"{i+1}: {paragraphs[idx]}\")\n",
    "    print(f\"distance {D[0][i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faeac92",
   "metadata": {},
   "source": [
    "Let's convert the above to a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd676d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_contexts(question, question_tokenizer, question_encoder, index, k = 5):\n",
    "    # Tokenize the question\n",
    "    question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "\n",
    "    # Encode the question to get the embedding\n",
    "    question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
    "\n",
    "    # Search the index to retrieve top k relevant contexts\n",
    "    D, I = index.search(question_embedding, k)\n",
    "\n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6445b0",
   "metadata": {},
   "source": [
    "## From retrival to final answer: decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77631f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c282899e6f1b47f8a685b47066e303ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beedbd4380cd4bc5a47e4485ea4f8e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6703398fc966497389510a8502d41438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910d39ccfaa04f2c9ac261b5d33ca627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e838c43ed6594134bf1776c4cdf58bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a12818764f4629ab3d5e7387f5aea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05945d210a3d43b4bde88fc331019282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159ad09",
   "metadata": {},
   "source": [
    "Now we compare the answer from the very same question: the first answer comes directly from the decoder (without context), the second uses the retrival process we have built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69d8fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_without_context(question):\n",
    "    # Tokenize the input question\n",
    "    inputs = tokenizer(question, return_tensors = 'pt', max_length = 1024, truncation = True)\n",
    "    \n",
    "    # Generate output directly from the question without additional context\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length = 150, min_length = 40, length_penalty = 2.0,\n",
    "                                 num_beams = 4, early_stopping = True, pad_token_id = tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    answer = tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "    return answer\n",
    "\n",
    "def generate_answer(question, contexts):\n",
    "    # Concatenate the retrieved contexts to form the input to GPT2\n",
    "    input_text = question + ' ' + ' '.join(contexts)\n",
    "    inputs = tokenizer(input_text, return_tensors = 'pt', max_length = 1024, truncation = True)\n",
    "\n",
    "    # Generate output using GPT2\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_new_tokens = 50, min_length = 40, length_penalty = 2.0,\n",
    "                                 num_beams = 4, early_stopping = True, pad_token_id = tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61efa42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: what is mobile policy?\n",
      "\n",
      "Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other\n",
      "Answer: what is mobile policy? 4.\tMobile Phone Policy The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes. Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations. The Mobile Phone Policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices.\n",
      "\n",
      "The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. Monitoring\n"
     ]
    }
   ],
   "source": [
    "question = \"what is mobile policy?\"\n",
    "answer = generate_answer_without_context(question)\n",
    "\n",
    "print(\"Answer:\", answer)\n",
    "\n",
    "_, I = search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5)\n",
    "top_contexts = [paragraphs[idx] for idx in I[0]] \n",
    "answer = generate_answer(question, top_contexts)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d969af",
   "metadata": {},
   "source": [
    "# 2) RAG with PyTorch (Similarity-based retrieval of static answers using BERT mean embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491d241",
   "metadata": {},
   "source": [
    "In this notebook 3) we want to understand if a song is ok for children, and for doing this we use a similarity-based retrival. We have:\n",
    "1. pre-defined set of questions `song_questions`;\n",
    "2. lyrics of a song as `sesame_street` and `my_shoe_lyrics`;\n",
    "3. list of predefined answers, not generated, which is `yes_responses`.\n",
    "\n",
    "**Very important**: notice the Q and A are associated, this is the reason for which the answers embedding will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e168751",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_questions = [\n",
    "    \"Does this song contain any violent themes, such as references to guns, killing, or physical aggression? Example: Does the song describe or promote physical violence, like fighting or shootings?\",\n",
    "    \"Are there any explicit lyrics or bad words used in this song that might be considered offensive or inappropriate? Example: Does the song use language commonly recognized as profanity or derogatory terms?\",\n",
    "    \"Is the overall content of this song suitable for children, considering its themes, language, and messages? Example: Are there elements in the song that could be deemed too mature or unsuitable for young listeners?\",\n",
    "    \"Does this song explicitly mention weapons, such as guns, knives, or other similar items? Example: Are specific types of weapons described or glorified in the lyrics?\",\n",
    "    \"Are the messages conveyed in this song positive and uplifting for children? Example: Does the song promote values like kindness, friendship, and positivity?\",\n",
    "    \"Does this song include any sexual content, references to sexual behavior, or suggestive language? Example: Are there lyrics that explicitly or implicitly discuss sexual themes or experiences?\",\n",
    "    \"Does this song offer any educational value, such as teaching the alphabet, basic math, or other learning content? Example: Are there educational segments in the song that could help children learn fundamental skills like the ABCs or counting?\",\n",
    "    \"Does this song promote emotional resilience and social skills among children? Example: Does the song include themes of overcoming challenges or building friendships?\"\n",
    "]\n",
    "\n",
    "sesame_street = \"\"\"\n",
    "Sunny day\n",
    "Sweepin' the clouds away\n",
    "On my way to where the air is sweet\n",
    "Can you tell me how to get\n",
    "How to get to Sesame Street?\n",
    "\n",
    "Come and play\n",
    "Everything's A-okay\n",
    "Friendly neighbors there\n",
    "That's where we meet\n",
    "Can you tell me how to get\n",
    "How to get to Sesame Street?\n",
    "\n",
    "It's a magic carpet ride\n",
    "Every door will open wide\n",
    "To happy people like you\n",
    "Happy people like\n",
    "What a beautiful\n",
    "\n",
    "Sunny day\n",
    "Sweepin' the clouds away\n",
    "On my way to where the air is sweet\n",
    "Can you tell me how to get\n",
    "How to get to Sesame Street?\n",
    "How to get to Sesame Street?\n",
    "How to get to Sesame Street?\n",
    "How to get to Sesame Street?\n",
    "How to get to Sesame Street?\n",
    "\"\"\"\n",
    "\n",
    "my_shoe_lyrics=\"\"\"Barney is a dinosaur from our imagination\n",
    "And when he's tall\n",
    "He's what we call a dinosaur sensation\n",
    "Barney's friends are big and small\n",
    "They come from lots of places\n",
    "After school they meet to play\n",
    "And sing with happy faces\n",
    "Barney shows us lots of things\n",
    "Like how to play pretend\n",
    "ABC's, and 123's\n",
    "And how to be a friend\n",
    "Barney comes to play with us\n",
    "Whenever we may need him\n",
    "Barney can be your friend too\n",
    "If you just make-believe him!\"\"\"\n",
    "\n",
    "yes_responses = [\n",
    "    \"Yes, this song contains violent themes, including references to guns, killing, or physical aggression, and is not suitable for children.\",\n",
    "    \"Yes, this song includes explicit lyrics or bad words that might be considered offensive or inappropriate for young audiences.\",\n",
    "    \"No, the overall content of this song is not suitable for children as it includes themes, language, and messages that are too mature or unsuitable for young listeners.\",\n",
    "    \"Yes, this song explicitly mentions weapons, such as guns and knives, which could be disturbing or inappropriate for children’s entertainment.\",\n",
    "    \"Yes, the messages conveyed in this song are positive and uplifting, promoting values like kindness, friendship, and positivity, beneficial for children.\",\n",
    "    \"Yes, this song includes sexual content and references to sexual behavior or suggestive language, which are inappropriate for a child-friendly environment.\",\n",
    "    \"Yes, this song offers significant educational value, including segments that teach the alphabet, basic math, and other learning content, making it both fun and educational for children.\",\n",
    "    \"Yes, this song promotes emotional resilience and social skills, incorporating themes about overcoming challenges and building friendships, which are essential for children's development.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "196c26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song(song):\n",
    "    # Remove line breaks from the song\n",
    "    song_new = re.sub(r'[\\n]', ' ', song)\n",
    "    \n",
    "    # Remove single quotes from the song\n",
    "    processed_song = [song_new.replace(\"\\'\", \"\")]\n",
    "    \n",
    "    return processed_song\n",
    "\n",
    "sesame_street= process_song(sesame_street)\n",
    "my_shoe_lyrics= process_song(my_shoe_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89aa6ab",
   "metadata": {},
   "source": [
    "## Tokenizer and Model (for both questions and context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13fa9ea",
   "metadata": {},
   "source": [
    "Why Use BERT Instead of DPR?\n",
    "Because:\n",
    "\n",
    "- The task is more semantic and classification-oriented:\n",
    "The goal is to check whether a piece of text has certain attributes (e.g., violent content), not to retrieve the most relevant document from a large corpus.\n",
    "\n",
    "- BERT is sufficient and more general-purpose for computing mean embeddings:\n",
    "It captures general semantic information well for tasks like similarity and classification.\n",
    "\n",
    "- DPR would be overkill for this use case:\n",
    "It's optimized for retrieval scenarios involving millions of documents and queries, not for small-scale semantic matching.\n",
    "\n",
    "- BERT is more stable on small datasets:\n",
    "Since it's not fine-tuned for any specific retrieval task, it performs more consistently across varied inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8bae0",
   "metadata": {},
   "source": [
    "`batch_encode_plus` method is used for tokenizing text. It automatically handles padding and truncation to ensure uniformity in input length, which is crucial for batch processing in models like BERT. `attention_mask`: Identifies which tokens should be focused on, differentiating real content from padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a58d9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281e25d",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7e94cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[CLS] this is an example sentence for bert embeddings. [SEP] how do you like it [SEP]\n"
     ]
    }
   ],
   "source": [
    "input_text = [(\"This is an example sentence for BERT embeddings.\", \"How do you like it \"),(\"There are other models\")]\n",
    "input_ids = tokenizer.batch_encode_plus(input_text,add_special_tokens=True,padding=True,truncation=True)\n",
    "print(input_ids['attention_mask'])\n",
    "text=tokenizer.decode(input_ids['input_ids'][0])\n",
    "print(text)\n",
    "\n",
    "input_ids_tensors = torch.tensor(input_ids['input_ids']).to(device)\n",
    "mask_tensors = torch.tensor(input_ids['attention_mask']).to(device)\n",
    "word_embding = bert_model(input_ids_tensors,mask_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a4d01",
   "metadata": {},
   "source": [
    "## Aggregate with mean for retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13c46c",
   "metadata": {},
   "source": [
    "Here, you'll compute aggregated mean embeddings for input sequences using the BERT model you just loaded. It processes each pair of token IDs and attention masks from the input data, extracts word embeddings for non-padded tokens, and calculates their mean. The result is a list of mean embeddings for each sequence, which is then concatenated into a single tensor. This process allows for the generation of simplified yet informative representations of the input sequences, useful for tasks like clustering, similarity search, or as input to downstream models. Each document must be under 512 tokens.\n",
    "\n",
    "Importantly, the output of this function, as for the case 2) with Hugging face has dimension [2,768].\n",
    "\n",
    "Below, we define a function that does the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02bca622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8c6809227542aaba1ad51a3427eeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids_tensor shape: torch.Size([1, 20]) torch.Size([1, 20])\n",
      "Word embeddings shape: torch.Size([20, 768])\n",
      "Number of zero padding embeddings: 0\n",
      "valid_embeddings_mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True])\n",
      "Word embeddings after zero padding embeddings removed: torch.Size([20, 768])\n",
      "Mean embedding shape: torch.Size([768])\n",
      "token_ids_tensor shape: torch.Size([1, 20]) torch.Size([1, 20])\n",
      "Word embeddings shape: torch.Size([20, 768])\n",
      "Number of zero padding embeddings: 14\n",
      "valid_embeddings_mask: tensor([ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Word embeddings after zero padding embeddings removed: torch.Size([6, 768])\n",
      "Mean embedding shape: torch.Size([768])\n",
      "All mean embeddings shape: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the mean embeddings for each input sequence\n",
    "aggregated_mean_embeddings = []\n",
    "\n",
    "# Loop over each pair of input_ids and attention_masks\n",
    "for token_ids, attention_mask in tqdm(zip(input_ids['input_ids'], input_ids['attention_mask'])):\n",
    "    # Convert list of token ids and attention mask to tensors\n",
    "    token_ids_tensor = torch.tensor([token_ids]).to(device)\n",
    "    attention_mask_tensor = torch.tensor([attention_mask]).to(device)\n",
    "    print(\"token_ids_tensor shape:\",token_ids_tensor.shape, attention_mask_tensor.shape)  # Print the shapes of the input tensors\n",
    "    with torch.no_grad():  # Disable gradient calculations for faster execution\n",
    "        # Retrieve the batch of word embeddings from the BERT model\n",
    "        embeddings = bert_model(token_ids_tensor, attention_mask=attention_mask_tensor)[0].squeeze(0)\n",
    "        print(\"Word embeddings shape:\", embeddings.shape)\n",
    "        \n",
    "        # Count and print the number of zero-padding embeddings\n",
    "        num_zero_paddings = (attention_mask_tensor == 0).sum().item()\n",
    "        print(\"Number of zero padding embeddings:\", num_zero_paddings)\n",
    "        \n",
    "        # Create a mask for positions that are not zero-padded\n",
    "        valid_embeddings_mask = attention_mask_tensor[0] != 0\n",
    "        print(\"valid_embeddings_mask:\",valid_embeddings_mask)\n",
    "        \n",
    "        # Filter out the embeddings corresponding to zero-padded positions\n",
    "        filtered_embeddings = embeddings[valid_embeddings_mask, :]\n",
    "        print(\"Word embeddings after zero padding embeddings removed:\", filtered_embeddings.shape)\n",
    "        \n",
    "        # Compute the mean of the filtered embeddings\n",
    "        mean_embedding = filtered_embeddings.mean(axis=0)\n",
    "        print(\"Mean embedding shape:\", mean_embedding.shape)\n",
    "    \n",
    "        # Append the mean embedding to the list, adding a batch dimension\n",
    "        aggregated_mean_embeddings.append(mean_embedding.unsqueeze(0))\n",
    "\n",
    "# Concatenate all mean embeddings to form a single tensor\n",
    "aggregated_mean_embeddings = torch.cat(aggregated_mean_embeddings)\n",
    "print('All mean embeddings shape:', aggregated_mean_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b83e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_embeddings(input_ids, attention_masks, bert_model=bert_model):\n",
    "    mean_embeddings = []\n",
    "    # Process each sequence in the batch\n",
    "    print('number of inputs',len(input_ids))\n",
    "    for input_id, mask in tqdm(zip(input_ids, attention_masks)):\n",
    "        input_ids_tensor = torch.tensor([input_id]).to(device)\n",
    "        mask_tensor = torch.tensor([mask]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Obtain the word embeddings from the BERT model\n",
    "            word_embeddings = bert_model(input_ids_tensor, attention_mask=mask_tensor)[0].squeeze(0)\n",
    "\n",
    "            # Filter out the embeddings at positions where the mask is zero \n",
    "            valid_embeddings_mask=mask_tensor[0] != 0 \n",
    "            valid_embeddings = word_embeddings[valid_embeddings_mask,:]\n",
    "            # Compute the mean of the filtered embeddings\n",
    "            mean_embedding = valid_embeddings.mean(dim=0)\n",
    "            mean_embeddings.append(mean_embedding.unsqueeze(0))\n",
    "\n",
    "    # Concatenate the mean embeddings from all sequences in the batch\n",
    "    aggregated_mean_embeddings = torch.cat(mean_embeddings)\n",
    "    return aggregated_mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0edca7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_emb(list_of_text, max_input = 512):\n",
    "    data_token_index  = tokenizer.batch_encode_plus(list_of_text, add_special_tokens=True,padding=True,truncation=True,max_length=max_input)\n",
    "    question_embeddings=aggregate_embeddings(data_token_index['input_ids'], data_token_index['attention_mask'])\n",
    "    return question_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bda2a1",
   "metadata": {},
   "source": [
    "## Embeddings and answer retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39cb2f",
   "metadata": {},
   "source": [
    "**Very important**: notice the Q and A are associated, this is the reason for which the answers embedding will not be used. In general, if the answers are not associated 1-1 to the questions, or if for each question there are multiple answers, then it should be useful to compute all the 3 products (Q-A, Q-song, A-song), and combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53ae11cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of inputs 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc7a0428bc944ebbba61dbf8087280f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of inputs 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc36e8d2ce148c59fc30883ffb87f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of inputs 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a0651fc34b414f95def376adfa296b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_questions = text_to_emb(song_questions) #torch.Size([8, 768])\n",
    "# embeddings_responses = text_to_emb(yes_responses) #torch.Size([8, 768]) --> not used\n",
    "\n",
    "#songs\n",
    "embeddings_sesame_street = text_to_emb(sesame_street) #torch.Size([1, 768])\n",
    "embeddings_my_shoe = text_to_emb(my_shoe_lyrics) #torch.Size([1, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16a4768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_QA(embeddings_questions, embeddings, n_responses = 3, distance = \"dot\"):\n",
    "    question_norms = torch.norm(embeddings_questions, dim=1, keepdim=True)\n",
    "    response_norms = torch.norm(embeddings, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "    # Calculate the dot product between the question embeddings and the provided embeddings (transpose of the second matrix for proper alignment).\n",
    "    dot_product = embeddings_questions @ embeddings.T\n",
    "\n",
    "    \n",
    "    # Calculate cosine similarity by dividing the dot product by the product of the magnitudes\n",
    "    cosine_similarity = dot_product / (question_norms * response_norms)\n",
    "\n",
    "    # Flatten the cosine similarity tensor to a 1D tensor for easier processing\n",
    "    cosine_similarity = cosine_similarity.reshape(-1)\n",
    "    \n",
    "    # Reshape the dot product results to a 1D tensor for easier processing.\n",
    "    dot_product = dot_product.reshape(-1)\n",
    "\n",
    "    if distance == \"dot\":\n",
    "        # Sort the indices of the dot product results in descending order (setting descending to False should be True for typical similarity tasks).\n",
    "        sorted_indices = torch.argsort(dot_product, descending = True)\n",
    "\n",
    "    if distance == \"cosine\":\n",
    "        sorted_indices = torch.argsort(cosine_similarity, descending =True)\n",
    "\n",
    "    # Convert sorted indices to a list for easier iteration.\n",
    "    sorted_indices = sorted_indices.tolist()\n",
    "\n",
    "    # Print the top 'n_responses' responses from the sorted list, which correspond to the highest dot product values.\n",
    "    for index in sorted_indices[:n_responses]:\n",
    "        print(yes_responses[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36cad733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the messages conveyed in this song are positive and uplifting, promoting values like kindness, friendship, and positivity, beneficial for children.\n",
      "Yes, this song offers significant educational value, including segments that teach the alphabet, basic math, and other learning content, making it both fun and educational for children.\n",
      "Yes, this song includes explicit lyrics or bad words that might be considered offensive or inappropriate for young audiences.\n"
     ]
    }
   ],
   "source": [
    "RAG_QA(embeddings_questions, embeddings_sesame_street, n_responses = 3, distance = 'dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957e7d2",
   "metadata": {},
   "source": [
    "## Retrival Augmented Generation (RAG) and Facebook AI Similarity (FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c9bed",
   "metadata": {},
   "source": [
    "**RAG** is a framework that helps optimize the output of LLMs without re-training the model, and by using an internal database of a company for example. To do this, RAG comprises two main components:\n",
    "1. **The retriver**: the retriver combine:\n",
    "    1. **encoded prompt**: a high-dimensional vectorial representation of the prompt (which is translated in a vector using a **question encoder**). The question encoder, at it ends, does an average;\n",
    "    2. **relevant context**: an 'internal' database (obtained using a **context encoder** from internal documents of the company). ;\n",
    "\n",
    "   The retrival combine the relevant context and the prompt matching similar vectors in the embedded spaces. As vector similarities, we can use the dot product for the magnitude and the cosine similarity for the direction.\n",
    "\n",
    "2. **The generator**: using the data from the retriver, it answers to the user using a **decoder** (use `BartForConditionalGeneration` and `BartTokenizer`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b870c",
   "metadata": {},
   "source": [
    "For the context encoder use `DPRContextEncoderTokenizer` from `transformers`, which reads list of tuples, and `DPRContextEncoder` .\n",
    "\n",
    "For the question encoder use `DPRQuestionEncoderTokenizer` and `DPRQuestionEncoder`\n",
    "\n",
    "Library for compute the distance importing `faiss`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56913b2e",
   "metadata": {},
   "source": [
    "## In-context learning and prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51a816",
   "metadata": {},
   "source": [
    "**In-context learning** is a method of doing prompt engineering, and in particular we give to the model demonstration of the task provided.\n",
    "- Advantages: \n",
    "    1. Does not require fine-tuning --> reduce time\n",
    "    2. Improve performances\n",
    "- Disadvantages:\n",
    "    1. Limited to what fit in-context (what example can I realistically include in the prompt?)\n",
    "    2. Complex tasks may require gradient steps and adjustments based on gradients\n",
    "**Prompt Engineering**: prompts are divided in instructions and context (necessary background to do the task). PI is about how to ask a LLM questions in the best way possible. It is crucial to:\n",
    "1. One-shot prompt: give one example of i.e. translation before asking to translate a sentence;\n",
    "2. Few-shot prompts: giving some example of sentiment analysis before asking a new one;\n",
    "3. Chain of thought: give an example and break it into steps for the solution to be effective;\n",
    "4. Self consistency: 'when I was 6 my sister was half my age. Now I am 50, what age is my sister? Provide N independent calculations and explanations, then determine the most consistent result'. The model at the end choose the most frequent answer.\n",
    "\n",
    "Where test the prompts? \n",
    "1. Playground;\n",
    "2. LangChain: uses prompt templates, which include few shot examples.\n",
    "3. HuggingFace;\n",
    "4. IBM AI classroom;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3025f0",
   "metadata": {},
   "source": [
    "## LangChain (chain of commands!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f879bb8",
   "metadata": {},
   "source": [
    "Components of LangChain:\n",
    "\n",
    "1. Language model: foundation of LLMs, using IBM, OpenAI, Google and Meta as primary language models\n",
    "2. Chat model: efficient conversation\n",
    "3. Chat message: efficient messages\n",
    "4. Prompt templates: translate user questions into clear instructions\n",
    "5. Output parser: transforms the output in suitable structured data\n",
    "\n",
    "\n",
    "We can use LangChain Documents to use RAG, and also to build applications (unifying chains, or sequence of calls!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b9dcb",
   "metadata": {},
   "source": [
    "In recent years, the development of Large Language Models (LLMs) like GPT-3 and GPT-4 has revolutionized the field of natural language processing (NLP). These models are capable of performing a wide range of tasks, from generating coherent text to answering questions and summarizing information. Their effectiveness, however, is not without limitations. One significant constraint is the context window length, which affects how much information can be processed at once. LLMs operate within a fixed context window, measured in tokens, with GPT-3 having a limit of 4096 tokens and GPT-4 extending to 8192 tokens. When dealing with lengthy documents, attempting to input the entire text into the model's prompt can lead to truncation, where essential information is lost, and increased computational costs due to the processing of large inputs.\n",
    "\n",
    "These limitations become particularly pronounced when creating a retrieval-based question-answering (QA) assistant. The context length constraint restricts the ability to input all content into the prompt simultaneously, leading to potential loss of critical context and details. This necessitates the development of sophisticated strategies for selectively retrieving and processing relevant sections of the document. Techniques such as chunking the document into manageable parts, employing summarization methods, and using external retrieval systems are crucial to address these challenges. Understanding and mitigating these limitations are essential for designing effective QA systems that leverage the full potential of LLMs while navigating their inherent constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trans_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
