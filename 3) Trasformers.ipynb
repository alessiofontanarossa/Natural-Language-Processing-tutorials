{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b50345",
   "metadata": {},
   "source": [
    "As it is (without anything downloaded and always with 1 epoch when needed), the running time of the whole notebook is (approximately) <span style=\"background-color: lightblue\"> 3 minutes</span>.\n",
    "\n",
    "<span style=\"background-color: yellow\"> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da8dfa8",
   "metadata": {
    "id": "2da8dfa8"
   },
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3665d493",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3665d493"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/gw8dmgsd6m11bg8nhrgpd3vr0000gn/T/ipykernel_19754/2932643662.py:15: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources            # package and dependency management\n",
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "########################## UTILITY AND SYSTEM ##########################\n",
    "\n",
    "import os                       # filesystem operations\n",
    "import csv                      # reading/writing CSV files\n",
    "import json                     # JSON parsing and serialization\n",
    "import math                     # basic math functions\n",
    "import random                   # random number generation\n",
    "import time                     # time-related functions\n",
    "import tempfile                 # temporary file management\n",
    "import tarfile                  # tar archive handling\n",
    "import io                       # input/output streams\n",
    "import pickle                   # object serialization\n",
    "import importlib                # dynamic import of modules\n",
    "import multiprocessing          # parallel process management\n",
    "import pkg_resources            # package and dependency management\n",
    "from copy import deepcopy       # deep copy of objects\n",
    "from pathlib import Path        # filesystem paths handling (cross-platform)\n",
    "\n",
    "########################## DOWNLOAD ##########################\n",
    "\n",
    "import requests                 # HTTP requests library\n",
    "import wget                     # file downloads from URLs\n",
    "from urllib.request import urlopen  # open URLs (alternative to requests)\n",
    "\n",
    "########################## VISUALIZATION ##########################\n",
    "\n",
    "import matplotlib.pyplot as plt # basic plotting library\n",
    "import plotly.graph_objs as go  # interactive plotting\n",
    "from tqdm.notebook import tqdm  # progress bars for loops in notebooks\n",
    "from pprint import pprint       # formatted pretty-printing of objects\n",
    "\n",
    "########################## DATAFRAME ##########################\n",
    "\n",
    "import numpy as np              # numerical arrays and operations\n",
    "import pandas as pd             # dataframes and data manipulation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "########################## TEXT PROCESSING ##########################\n",
    "\n",
    "import re                      # regular expressions\n",
    "import string                  # string constants and operations\n",
    "from itertools import chain, islice  # advanced iteration and chaining\n",
    "\n",
    "########################## TOKENIZATION ##########################\n",
    "\n",
    "from collections import Counter, OrderedDict  # frequency counts and ordered dictionaries\n",
    "import nltk                                   # natural language processing toolkit\n",
    "from nltk.tokenize import word_tokenize       # word tokenization\n",
    "import spacy                                  # advanced NLP (tokenization, parsing)\n",
    "from torchtext.data.utils import get_tokenizer       # torchtext tokenizers\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator # build vocabulary from iterator\n",
    "\n",
    "########################## DATASET AND DATALOADER ##########################\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split   # datasets and data loading utilities\n",
    "from torch.nn.utils.rnn import pad_sequence                      # padding variable-length sequences\n",
    "from datasets import load_dataset, DatasetDict                   # HuggingFace datasets loading\n",
    "from torchtext.datasets import AG_NEWS                           # torchtext built-in datasets\n",
    "\n",
    "########################## PYTORCH AND DEEP LEARNING ##########################\n",
    "\n",
    "import torch                             # PyTorch main library\n",
    "from torch import nn, Tensor             # neural network modules and tensors\n",
    "from torch.nn import CrossEntropyLoss    # common loss function for classification\n",
    "from torchsummary import summary as torchsummary\n",
    "from torchinfo import summary as torchinfosummary\n",
    "\n",
    "########################## WORD EMBEDDING ##########################\n",
    "\n",
    "from torchtext.vocab import GloVe        # pretrained GloVe embeddings\n",
    "# from gensim.models import Word2Vec     # word2vec embeddings from corpus (commented out)\n",
    "\n",
    "########################## HUGGING FACE ##########################\n",
    "\n",
    "import transformers                      # transformers library core\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,     # GPT-2 tokenizer and model\n",
    "    BertTokenizer, BertTokenizerFast, BertConfig, BertForMaskedLM,  # BERT components\n",
    "    XLNetTokenizer,                     # XLNet tokenizer\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline,                          # easy pipelines for inference\n",
    "    AutoTokenizer,                    # auto tokenizer loader\n",
    "    AutoModelForCausalLM, GPT2ForSequenceClassification,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer,  # training utilities\n",
    "    set_seed, GenerationConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BertModel                        # BERT base model\n",
    ")\n",
    "from datasets import DatasetDict         # HuggingFace dataset dictionaries\n",
    "\n",
    "######################### TRL & PEFT (TRAINING & PARAMETER EFFICIENT FINE-TUNING) ##########################\n",
    "\n",
    "# from trl import (\n",
    "#     SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM,\n",
    "#     DPOConfig, DPOTrainer,\n",
    "#     RewardTrainer, RewardConfig\n",
    "# )\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torchmetrics import Accuracy        # metrics for evaluation\n",
    "\n",
    "########################## RAG ##########################\n",
    "\n",
    "from transformers import (\n",
    "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n",
    "    DPRContextEncoder, DPRContextEncoderTokenizer\n",
    ")\n",
    "import faiss                              # similarity search library\n",
    "\n",
    "########################## EVALUATION ##########################\n",
    "\n",
    "import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca7a467",
   "metadata": {
    "executionInfo": {
     "elapsed": 357760,
     "status": "aborted",
     "timestamp": 1752786900757,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "bca7a467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which device we are on: cpu\n"
     ]
    }
   ],
   "source": [
    "def accelerator(where = \"mps\"):\n",
    "    if where == \"mps\":\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "    if where == \"cuda\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "    if where == \"cpu\":\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "\n",
    "device = accelerator(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc4aa3",
   "metadata": {},
   "source": [
    "# A) CONCEPTS: Sequence-to-sequence, Positional Encoding, Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b958fc",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2b1f8",
   "metadata": {},
   "source": [
    "The aim here is to **transform a sequence into another**, and it is possibile that the first sequence has a different lenght from the second. They are used for translation, chatbots, code generation, image generation and text summarization. They overclass a problem with BOW, that is that 'the dog bites the cat' and 'the cat bites the dog' has the same BOW ---> <span style=\"background-color: yellow\">Positional Encoding</span>! Moreover, they want to manage input of different lenght and **keep a global memory** ---> <span style=\"background-color: yellow\">Attention Mechanism</span>, contrarily to word-to-vec and N-grams, which have a local memory. These models use:\n",
    "\n",
    "1. RNN, where the hidden layer is used for memory, concatenating the (outputs of) input layer at time $t$ and the hidden layer at $t-1$ to connect past and future. The input layer is given as input to the next step.\n",
    "\n",
    "RNNs have short memory and are challenging to train. Thus we use:\n",
    "\n",
    "2. Gated RNN: \n",
    "3. LSTM:\n",
    "4. Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01161c4a",
   "metadata": {},
   "source": [
    "Since sequence-to-sequence may not have the same dimensions (for example a sentence $x$ in input has lenght $T_x$ and the output $y$ has $T_y$), we introduce encoder-decoder:\n",
    "\n",
    "![Logo OpenAI](https://www.interdb.jp/dl/part03/fig-13-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef6b8f",
   "metadata": {},
   "source": [
    "Here $x^{(t)}$ is the $t$-th token from the sentence $x$. The output of RNN on $x^{(1)}$, which is the hidden state $h^{(1)}$, is used as input for the next RNN and so on until the context $h^{(T_x)}$, which is the last hidden state. Notice that the outputs are not used in the encoder, whilst they are saved in the decoder part, which produces predictions one-token-per-time. Before an encoder RNN cell (blue) we can always insert an embedding layer, and before and after a RNN cell we can always insert an embedding and a linear layer, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f2f09",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5f529",
   "metadata": {},
   "source": [
    "It is needed to distinguish 'He painted the car red', which is different from 'He painted the red car', but the vector representation is the same. Thus, we need\n",
    "1. positional encoding: given a sentence with $N$ tokens ($pos=0 ,\\ldots,N-1$), $h$ in the hidden dimension ($i=0,\\ldots,h/2 -1$), we add \n",
    "$$ PE(pos,2i)=\\sin\\Big(\\frac{pos}{10000^{2i/h}}\\Big)\\,,\\quad  PE(pos,2i+1)=\\cos\\Big(\\frac{pos}{10000^{2i/h}}\\Big)\\,,\\quad pos=0 ,\\ldots,N-1\\,,\\quad i=0,\\ldots,h/2 -1\\,,$$ \n",
    "to the embedding value of the token, position per position. Occasionally, for practicality, it is also useful to add padding to arrive to the vocabulary lenght. **We use the sine and cosine because they are limited functions, and they do not explode for very very long sequences**. Simple example, in which then you have to perform the sum:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "\n",
    "### Example: h = 4 and N = 3\n",
    "\n",
    "| Word        | Col1 | Col2 | Col3 | Col4 |\n",
    "|-------------|------|------|------|------|\n",
    "| transformers| 0.2  | 0.4  | 0.1  | 0.3  |\n",
    "| are         | 0.5  | 0.2  | 0.7  | 0.9  |\n",
    "| awesome     | 0.8  | 0.6  | 0.4  | 0.2  |\n",
    "\n",
    "</td>\n",
    "    <td>\n",
    "\n",
    "### Positional encoding\n",
    "\n",
    "| Word        | PE0  | PE1 | PE2  | PE3  |\n",
    "|-------------|------|------|------|------|\n",
    "| transformers| $\\sin\\Big(\\frac{0}{10000^{2i/h}}\\Big)=$ 0  | $\\cos\\Big(\\frac{0}{10000^{2i/h}}\\Big)=$ 1  | $\\sin\\Big(\\frac{0}{10000^{2i/h}}\\Big)=$ 0  | $\\cos\\Big(\\frac{0}{10000^{2i/h}}\\Big)=$ 1  |\n",
    "| are         | $\\sin\\Big(\\frac{1}{10000^{2i/h}}\\Big)=$ 0.84 | $\\cos\\Big(\\frac{1}{10000^{2i/h}}\\Big)=$ 0.54 | $\\sin\\Big(\\frac{1}{10000^{2i/h}}\\Big)=$ 0.01 | $\\cos\\Big(\\frac{1}{10000^{2i/h}}\\Big)=$ 0.99  |\n",
    "| awesome     | $\\sin\\Big(\\frac{2}{10000^{2i/h}}\\Big)=$ 0.90 | $\\cos\\Big(\\frac{2}{10000^{2i/h}}\\Big)=$ -0.41 | $\\sin\\Big(\\frac{2}{10000^{2i/h}}\\Big)=$ 0.02 | $\\cos\\Big(\\frac{2}{10000^{2i/h}}\\Big)=$ 0.99  |\n",
    "\n",
    "</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "2. Dinamical positional encoding: the positional encoding are learnable parameter (GPT)\n",
    "3. Segment encoding: used in BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f0ff0",
   "metadata": {},
   "source": [
    "## Attention Mechanism idea (without transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3a9b4",
   "metadata": {},
   "source": [
    "The attention mechanism with queries, keys, and values addresses the long-distance dependency problem by allowing each word to directly attend to all other words in the sequence, regardless of their position. Instead of relying on sequential processing, it computes weighted sums (attention scores) between all word pairs, enabling the model to capture relevant context from distant words in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b014de9",
   "metadata": {},
   "source": [
    "We can use OHE vectors or embedded vectors. Here **K=keys**,**Q=queries** and **V=values** and we have to think to a pPython dictionary, which is dict={K:V} and we acceed to it by doing a query dict[q] (q is one among the K):\n",
    "\n",
    "**OHE**:\n",
    "1. $k_{le} = (0,0,0,1,0,0)^t$, $k_{chat}=(1,0,0,0,0,0)^t$, $k_{est}=(0,1,0,0,0,0)^t$, $k_{sous}=(0,0,0,0,1,0)^t$, $k_{la}=(0,0,1,0,0,0)^t$, $k_{table}=(0,0,0,0,0,1)^t$ and we form the matrix **$K_{n \\times d_k}$** (each row is a $k_i$), where **$d_k$** is the dimension of the keys vocabulary and **$n$** is the number of token;\n",
    "2. same for the queries vectors, and we build **$Q_{n \\times d_k}=K_{n \\times d_k}$**;\n",
    "3. Similar for the values, and the matrix will be **$V_{n \\times d_v}$**, where **$d_v$** is the dimension of the keys vocabulary\n",
    "\n",
    "Then for a specific query $q_i$, we have\n",
    "\n",
    "$$\\text{Attention}(q_i, K,V) \\equiv (h_i^t)_{1 \\times d_v}= (q_i^t)_{1 \\times d_k} \\cdot (K^t)_{d_k \\times n} \\cdot (V)_{n \\times d_v}\\,.$$\n",
    "\n",
    "Due to orthogonality, the vector $h_i$ is exactly equal to $v_i$, so this attention is a way to perform a query on a dictionary rapidly. To retrieve the word from the OHE we do\n",
    "\n",
    "$$ \\text{predicted word} = \\argmax_{\\text{index}}(h_i^t )\\,.$$\n",
    "\n",
    "**Word Embedding**:\n",
    "\n",
    "Everything is very similar, but **$K_{n \\times h_k}$** and **$V_{n \\times h_v}$**, where **$h_{k,v}$** are the hidden dimensions of keys and values. Now we use a softmax to mimic the behaviour of OHE vectors:\n",
    "\n",
    "$$\\text{Attention}(q_i, K,V) \\equiv (h_i^t)_{1 \\times d_v}= \\underbrace{\\text{softmax}\\Big[(q_i^t)_{1 \\times d_k} \\cdot (K^t)_{d_k \\times n}\\Big]}_{\\text{Attention Score!}} \\cdot (V)_{n \\times d_v}\\,.$$\n",
    "The attention score measures the semantic similarity between words! The piece in $\\Big[\\quad\\Big]$ is like $(0.01, 0.02,---,0.99,---,0.001)$. Similarly\n",
    "\n",
    "$$ \\text{predicted word} = \\argmax_{\\text{index}}(h_i^t )\\,.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a0400",
   "metadata": {},
   "source": [
    "If instead of word embedding we want to embed sequences, everything remains the same but we replace $q_i$ with an aggregate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cc614",
   "metadata": {
    "id": "e42cc614"
   },
   "source": [
    "# B) CONCEPTS: Transformers and its attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6eb0b6",
   "metadata": {},
   "source": [
    "## Transformer architecture and PyTorch classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3861d15",
   "metadata": {},
   "source": [
    "The following is a typical representation of the architecture of encoder - decoder. In particular, this is a 'single cell' or 'single temporal step', so in a real encoder there is a number of cell like this one. The 'outputs' under the decoder is the generated token from the decoder at the previous step! Finally, this gray cell is a type of RNN. Add & norm is useful to reduce problems with gradients. Notice that there are three types of attentions:\n",
    "1. <span style=\"background-color: orange\">Multi-Head Self-Attention in the Encoder</span>\n",
    "2. <span style=\"background-color: orange\">Masked Multi-Head Self-Attention in the Decoder</span>: Masked self-attention means that the model only attends to the previous tokens in the sequence for predicting the next token.\n",
    "3. <span style=\"background-color: orange\">Encoder-Decoder Multi-Head Cross-Attention in the Decoder</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f83414",
   "metadata": {},
   "source": [
    "| Phase                    | Query (Q)                         | Key (K)                             | Value (V)                           |\n",
    "|--------------------------|-----------------------------------|-------------------------------------|-------------------------------------|\n",
    "| Encoder self-attention   | from the encoder input            | from the encoder input              | from the encoder input              |\n",
    "| Decoder self-attention   | from the decoder input (partial)  | from the same decoder input         | from the same decoder input         |\n",
    "| Encoder-Decoder attention| **from the decoder**              | **from the encoder final output**   | **from the encoder final output**   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53f80c",
   "metadata": {},
   "source": [
    "Consider an **encoder** in the figure. The inputs are in the form <span style=\"background-color: yellow\">([tl = tokens_lenghts, bs = number of samples in a batch = batch_size])</span> , ans recall that all the sequences in a batch must have the same number of tokens (eventually, use padding). Then we use an `nn.Embedding` layer, so after the pink we have <span style=\"background-color: yellow\">([tl, bs, ed = embeding_dimension])</span>. The positional encoding does not change the dimensions, and also the ancoder. So the output of the encoder, which is called **contextual embedding** is <span style=\"background-color: yellow\">([tl, bs, ed])</span>. Then usually one takes ` mean(dim = 0)` to arrive to <span style=\"background-color: yellow\">([bs, ed]) </span>, and finally the linear layer (for example a classifier) arrives to the number of classes <span style=\"background-color: yellow\">([bs, num_classes]) </span> in the classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14772c92",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*376uJu_fc_uR8H3X.png\" alt=\"Logo OpenAI\" width=\"900\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cadf0a",
   "metadata": {},
   "source": [
    "To understand the above picture (where recall that the cell is a single RNN and not the full encoder) and the PyTorch classes used, we go through the following steps:\n",
    "\n",
    "1. **Self-attention**, which is at the hearth of the encoder-decoder. This can be done defining an Head class manually, but more properly there is the `nn.MultiheadAttention` method from PyTorch;\n",
    "2. **Positional Encoding**\n",
    "3. Finally a full encoder-decoder! For this we have many classes\n",
    "\n",
    "| Class                          | Main Purpose                                                             | Internal Components                                                                 | When to Use                                                              |\n",
    "|-------------------------------|---------------------------------------------------------------------------|--------------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| `nn.Transformer`              | Full Transformer model (Encoder + Decoder).                              | Includes `TransformerEncoder` + `TransformerDecoder`.                               | For sequence-to-sequence tasks like translation or summarization.        |\n",
    "| `nn.TransformerEncoderLayer` | A **single** encoder layer: self-attention + feed-forward + normalization.| `MultiheadAttention` (self-attn), dropout, LayerNorm, feed-forward (FFN), residuals. | To manually build or inspect an encoder block.                           |\n",
    "| `nn.TransformerEncoder`      | A **stack** of encoder layers.                                           | Repeats `TransformerEncoderLayer` N times (can share or not share weights).         | For pure encoder tasks like text classification, embeddings (e.g. BERT). |\n",
    "| `nn.TransformerDecoderLayer` | A **single** decoder layer: self-attn + encoder-decoder attn + FFN.       | Self-attn, cross-attn (to encoder), dropout, LayerNorm, FFN, residuals.              | To inspect or customize a decoder block manually.                        |\n",
    "| `nn.TransformerDecoder`      | A **stack** of decoder layers.                                           | Repeats `TransformerDecoderLayer` N times.                                          | Use when building the decoder part of a full Transformer model.          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17d8d9",
   "metadata": {},
   "source": [
    "In particular, notice that for `nn.TransformerEncoderLayer` is a single RNN, so the gray cell, and also that it contains already the multiheadAttention mechanism. It can be accessed using\n",
    "```\n",
    "layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "print(layer.self_attn)\n",
    "```\n",
    "\n",
    "A full decoder is a stack of N-RNN, which is nn.transformerEncoder!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd1d67",
   "metadata": {},
   "source": [
    "Let's see them (the classes) one-by-one:\n",
    "\n",
    "```\n",
    "multihead_attention = nn.MultiheadAttention(embed_dim, num_heads,batch_first = False)\n",
    "query = torch.rand((seq_length, batch_size, embed_dim))\n",
    "key = torch.rand((seq_length, batch_size, embed_dim))\n",
    "value = torch.rand((seq_length, batch_size, embed_dim))\n",
    "attention_output, _ = multihead_attn(query, key, value)\n",
    "```\n",
    "\n",
    "```\n",
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((seq_length_in, batch_size, embed_dim))\n",
    "tgt = torch.rand((seq_length_out, batch_size, embed_dim))\n",
    "out = transformer_model(src, tgt)\n",
    "```\n",
    "\n",
    "```\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model = embed_dim, nhead = num_heads) # one encoder\n",
    "\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers) # stack of num_layers encoders\n",
    "src = torch.rand((seq_length_in, batch_size, embed_dim))\n",
    "encoded = transformer_encoder(src)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48998883",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX05RNEN/Tokenization%20-%20Color.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a01fa",
   "metadata": {
    "id": "660a01fa"
   },
   "source": [
    "## Attention in the Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d6c1b",
   "metadata": {},
   "source": [
    "Now we are going to expand on the orange boxes, so we see how the attention mechanism works within a transformer:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*376uJu_fc_uR8H3X.png\" alt=\"Logo OpenAI\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecccf8",
   "metadata": {},
   "source": [
    "### Self-Attention in the Encoder and in the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1df65",
   "metadata": {},
   "source": [
    "With reference to the figure above, we have that:\n",
    "\n",
    "\n",
    "1. The inputs are the tokens of the input sequence, encapsulated in a $n$-dimensional vector $X_n = (x_1,\\ldots,x_n)$. Each $x_i$ is the result of $\\text{vocab}(\\text{token}_i)$, where the vocabulary has lenght $|\\text{voc}|$. For example $X_{n=4}=[4,17,23,6]$;\n",
    "\n",
    "\n",
    "\n",
    "2. Then there is a (or a system of) linear layers to go to the embedding space. These are learnable parameters, or you can use a pretrained model as GloVe. As for the usual embeddings (`nn.EmbeddingBag`), what happens in the \"$E\\cdot X$\" step is that we use a matrix $E_{ |\\text{voc}| \\times d_{\\text{embed}}}$ which acts as \n",
    "\n",
    "$$\"E\\cdot X\" \\equiv \\begin{pmatrix} \n",
    "E[x_1] \\\\\n",
    "\\ldots \\\\\n",
    "E[x_n]\n",
    "\\end{pmatrix} = \\begin{pmatrix} \n",
    "E_{x_1,:} \\\\\n",
    "\\ldots \\\\\n",
    "E_{x_n,:}\n",
    "\\end{pmatrix} = \\begin{pmatrix} \n",
    "\\text{row }x_1\\text{ of }E \\\\\n",
    "\\ldots \\\\\n",
    "\\text{row }x_n\\text{ of }E\n",
    "\\end{pmatrix}\\equiv X'_{n \\times d_{\\text{embed}}}\\,;$$\n",
    "\n",
    "3. The positional encoding is added, to obtain $X''_{n \\times d_{\\text{embed}}} = X'_{n \\times d_{\\text{embed}}} +\\text{pos-enc}_{n \\times d_{\\text{embed}}}$. From now on the pos-enc will be omitted for simplicity;\n",
    "\n",
    "\n",
    "4.  The new vector $X''_{n \\times d_{\\text{embed}}}$ is then splitted using three different projection matrices (learnable) $ W^K \\in \\mathbb{R}^{d_k \\times d_{\\text{embed}} } $, $ W^V \\in \\mathbb{R}^{d_v\\times d_{\\text{embed}} } $ and $ W^Q \\in \\mathbb{R}^{d_k \\times d_{\\text{embed}} } $, for the keys, queries and values (this step is the represented in the figure by the three rows at the beginning pf the encoder or decode). Th parameters $d_{k,v}$ are again hyperparameters. We get, for example, \n",
    "\n",
    "\n",
    "$$ K_{n\\times d_k} = X''_{n \\times d_{\\text{embed}}} \\cdot \\big[(W^K_{d_k \\times d_{\\text{embed}}})^t\\big]_{d_{\\text{embed}}\\times d_k}\\,,$$\n",
    "\n",
    "$$ V_{n\\times d_v} = X''_{n \\times d_{\\text{embed}}} \\cdot \\big[(W^V_{d_v \\times d_{\\text{embed}}})^t\\big]_{d_{\\text{embed}}\\times d_v}\\,,$$\n",
    "\n",
    "$$ Q_{n\\times d_k} = X''_{n \\times d_{\\text{embed}}} \\cdot \\big[(W^Q_{d_k \\times d_{\\text{embed}}})^t\\big]_{d_{\\text{embed}}\\times d_k}\\,.$$\n",
    "\n",
    "\n",
    "5. The Attention and attention score are computed for each query token $q^{(i)}$, which is a single row of the matrix $Q_{n\\times d_k}$, and is thus $(q^{(i)})_{_{1\\times d_k}}$. The output of the self-attention for each token is a new representation $h^{(i)}$ of the token itself (as a vector $1\\times d_v$), which contain its meaning in relation to all the other tokens (the context). The used formulas are:\n",
    "\n",
    "$$\\text{Attention}(q^{(i)}, K,V) \\equiv (h^{(i)})_{1 \\times d_v}= \\underbrace{\\text{softmax}\\bigg[\\frac{(q^{(i)})_{1 \\times d_k} \\cdot (K^t)_{d_k \\times n}}{\\sqrt{d_k}}\\bigg]}_{\\text{Attention score!}} \\cdot V_{n \\times d_v}\\,,$$\n",
    "$$\\text{Attention}(Q, K,V) \\equiv H_{n \\times d_v}= \\underbrace{\\text{softmax}\\bigg[\\frac{Q_{n \\times d_k} \\cdot (K^t)_{d_k \\times n}}{\\sqrt{d_k}}\\bigg]}_{\\text{Attention score!}} \\cdot V_{n \\times d_v}\\,,$$\n",
    "\n",
    "6. After the attention, the output is sent \n",
    "    - to a feed-forward system (light-blue cell in the figure) for the encoder;\n",
    "    - to the cross-attention mechanism in the decoder;\n",
    "\n",
    "⚠️ Note: the output of an encoder block is **not a prediction**. It is a contextualized representation of the input tokens. These representations are passed through all encoder blocks and finally sent to the decoder, which uses them to generate predictions.\n",
    "\n",
    "⚠️ Note: the self-attention in the decoder is masked, so that the decoder only focuses the attention to the previous tokens in the sequence (to generate!). See the training section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9a229",
   "metadata": {},
   "source": [
    "### Cross-attention Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e9460a",
   "metadata": {},
   "source": [
    "Everything as in the previous subsection, but:\n",
    "- Key and Value comes from the final output of the stack of encoder blocks. By this, we mean that we have the context (final output of encoder) and we split as before in Key and Value matrices;\n",
    "- The Query matrix is the output of the decoder self-attention, so the new query is $H_{n_{\\text{dec}} \\times d_v}$\n",
    "\n",
    "⚠️ Note: the dimensions should coincide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72061fe9",
   "metadata": {},
   "source": [
    "## Multi-head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2facc8",
   "metadata": {
    "id": "6e2facc8"
   },
   "source": [
    "Now we can use a number of heads $n_h$, which are a $n_h$ different layers with weights $W_{i=1,\\ldots,n_h}$ for $\\{K,V,Q\\}$. The output of the layers will be concatenated and then goes to a final ouput layer $W^O$. So, for example,\n",
    "\n",
    "$$ \\text{MultiHead}(Q,K,V)=Concat(\\text{head}_{1}​,\\ldots,\\text{head}_{n_h}​)W^O\\,,\\quad \\text{head}_{i}= \\text{softmax}\\bigg[\\frac{(Q_i^t)_{m \\times d_k} \\cdot (K_i^t)_{d_k \\times n}}{\\sqrt{d_k}}\\bigg] \\cdot (V_i)_{n \\times d_v}\\,.$$\n",
    "\n",
    "with, for example,\n",
    "\n",
    "$$V_i = E_{\\text{src}}\\cdot W_i^V\\,,\\quad i=1,\\ldots,n_h\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3bbb3",
   "metadata": {},
   "source": [
    "# C) CONCEPTS: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299b5ae",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*376uJu_fc_uR8H3X.png\" alt=\"Logo OpenAI\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93658a",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924d173",
   "metadata": {},
   "source": [
    "Important difference between training and predicting using decoders: **Teacher Forcing**:\n",
    "\n",
    "1. During **prediction/inference** the flow is $$ \\underbrace{\\text{token}_0 = x_0}_{\\text{precisely: } \\text{vocab}(\\text{token}_0) = x_0} \\rightarrow \\text{input embedding} \\rightarrow \\text{decoder} \\rightarrow \\text{prevision representation } \\hat{x}_0 \\rightarrow \\text{output layer with output } z_1 \\rightarrow \\widehat{\\text{word}}_1= \\argmax_i(z_1)\\,.$$ Notice that usually <span style=\"background-color: yellow\"> $x_0=$ index of < BOS > </span>. Then $$ \\widehat{\\text{word}}_1 \\rightarrow \\text{input embedding}\\,.$$ Recall also that in general, to the input layer at the step $t$, are sent \n",
    "\n",
    "$$\\text{concat}[x_0,\\widehat{\\text{word}}_1 , \\ldots ,\\widehat{\\text{word}}_{t-1}]\\,.$$\n",
    "\n",
    "2. During **training** instead, to the input embedding are sent all the true tokens $\\text{src}\\equiv(x_0,\\ldots, x_{n-1})$ <span style=\"background-color: yellow\">at the same time (for time efficency!) </span> and we want to predict $\\text{tgt}\\equiv(x_1,\\ldots, x_{n})$ (these are the labels!), which is the very same sentence shifted by one position. The decoder, at $t=1$, predicts the logits $z_1$ (in the same notation as above). Then the loss is computed as a the Cross-entropy loss between the logits $z_1$ and the ground truth label $x_1\\in \\text{tgt}$. The problem is that sending all the $x_i \\in \\text{src}$ to the input embedding layer, there can be correlation with future words, in the sense that the algorithm can learn to predict the next word, using the next word instead of the present one. For this reason, within each decoder block a causal mask mechanism is introduced, so that even if we really send all $(x_0,\\ldots, x_{n-1})\\in \\text{src}$ at the same time, only the 'not future ones' count:\n",
    "    - **causal mask mechanism**: like positional encoding (in the sense that is added), but to the scaled attention scores you add a 'look-ahed mask' which has -inf values for future tokens prediction and zero otherwise. Then the softmax kills the 'future' values. The masking is usually implemented using the pyTorch class `generate_square_subsequent_mask()`. <span style=\"background-color: yellow\"> This is added to the $ \\text{src}$ sequence! </span>. To the $ \\text{tgt}$ sequence we add another type of mask, because there we do not want that the attention mechanism look to the padded values.\n",
    "\n",
    "The function `generate_square_subsequent_mask()` works like this:\n",
    "\n",
    "- The first line generates a square matrix with True in the upper triangular part (diagonal included!), and then transpose it, and then the .float() inserts 0 on the false. So at the end of the first line we have\n",
    "```\n",
    "tensor([[1., 0., 0.],\n",
    "        [1., 1., 0.],\n",
    "        [1., 1., 1.]])\n",
    "```\n",
    "- The second line replace the 0 in mask with -inf and the 1 with 0, this getting\n",
    "```\n",
    "tensor([[0., -inf, -inf],\n",
    "        [0., 0., -inf],\n",
    "        [0., 0., 0.]])\n",
    "```\n",
    "- The function is\n",
    "```\n",
    "def generate_square_subsequent_mask(size, device = device):\n",
    "    mask = (torch.triu(torch.ones((size, size), device = device)) == 1).transpose(0, 1).float() #\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "```\n",
    "\n",
    "Notice that $\\text{word}_t$ is not the real word, but it is the index of that word in the vocabulary. As a consequence, the loss is computed between ground truth integers and logits (as for a standard classification problem). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6c98e",
   "metadata": {},
   "source": [
    "**GPT vs chat GPT**\n",
    "\n",
    "GPT (Generative Pretrained Transformer) is a decoder-only model because it is trained using a causal language modeling objective, where the goal is to predict the next token in a sequence given the previous tokens. During training, the input sequence is shifted to the right, and the model learns to generate output tokens autoregressively, one at a time. This process allows GPT to generate coherent and contextually relevant text based on the given input prompt.\n",
    "GPT is a family of large-scale transformer-based language models trained on diverse internet text data. GPT models are designed for a wide range of natural language processing tasks, such as text generation, translation, summarization, and question-answering. They generate responses based on the input text (prompt) but do not maintain a consistent conversation history.\n",
    "\n",
    "On the other hand, ChatGPT is a fine-tuned version of the GPT model, specifically designed for conversational AI applications. It is trained to maintain a consistent conversation history and generate contextually relevant responses, making it more suitable for chatbot-like interactions. ChatGPT excels at understanding and generating human-like dialogues, providing coherent and engaging responses in a conversational setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c1da7",
   "metadata": {},
   "source": [
    "**SOURCE OF CONFUSION on GPT**\n",
    "\n",
    "A priori, one expects to use `nn.TransformerDecoderLayer` when constructing a decoder BUT `nn.TransformerDecoderLayer` is built to live in an encoder-decoder system, and for this reason it expects as input both self-attention (outputs of the decoder) AND cross-attention (outputs of the encoder). Thus:\n",
    "1. If we are building a lonely decoder, which uses self-attention and causal mask WITHOUT cross-attention, we should use `nn.TransformerEncoderLayer`WITH ALSO the specification of the causal mask (defined by hand);\n",
    "2. If we are building an encoder-decoder system, the decoder has a self-attention mechanism and a cross-attention mechanism. Here we use `nn.TransformerDecoderLayer`, which has (for the self-attention part) an integrated causal mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdeacc8",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02172e",
   "metadata": {},
   "source": [
    "It utilizes **entire sequences** simultaneously, **enhancing comprehension** of the semantic relations, using both sides of the target word (**bidirectional**). It is not designed for text generation. Instead, BERT is used for understanding tasks such as sentiment analysis and question.answering, where it performs classification or span prediction.\n",
    "\n",
    "The idea is to mask some known words (15% of the text), or **MLM: Masked language Modelling**, where 'mask' must not be confused with the causal mask of decoder. The encoder must predict them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af190349",
   "metadata": {},
   "source": [
    "Usally BERT is **PRETRAINED** using **MLM** and **NSP: Next Sentence Prediction** on vast corpora. The motivation behind unlabeled pretraining transformers is to address the limitations of traditional approaches that require significant amounts of labeled data for each specific task. Pretraining leverages the abundance of **unlabeled text data** available on the internet and facilitates transfer learning, where knowledge learned from one task can be transferred to aid in solving other related tasks.\n",
    "\n",
    "**NSP**\n",
    "\n",
    "This is a two classes (y=0/1) classification problem, where we take a complete sequence, we split it in two, take the first part as input and the second as target with y=1. Then give to the model other second parts sentences with y=0. In this way, training the model to the binary classification, BERT learns the logical relation between sentences, and not only among tokens. The complete sentence is modified adding < CLP > at the beginning and < SEP > at the end of the first (input) sentence.\n",
    "\n",
    "<span style=\"background-color: yellow\"> MLM: understand relations between words </span> and <span style=\"background-color: yellow\">NSP: enables the model to learn sentence-level relationships </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de56de",
   "metadata": {},
   "source": [
    "### MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e490c59",
   "metadata": {},
   "source": [
    "The process in MLM is the following. We select:\n",
    "- 15% of the tokens in the dataset <span style=\"background-color: orange\">for prediction </span> (called **masked positions**). Of this 15%:\n",
    "    1. 80% of tokens are substitued with `<MASK>`;\n",
    "    2. 10% is substitued with **random tokens**;\n",
    "    3. 10% is left unaltered;\n",
    "    - With respect to the total dataset, this number corresponds to 15% x 80% = 12%, 1.5% and 1.5%, respectively. For this subset, the label is exactly the original text;\n",
    "- the remaining 85% of the tokens left unaltered have as labels `<PAD>`, so that they do not contribute to the loss.\n",
    "\n",
    "|                  | 12% (MASK)       | 1.5% (Random Token) | 1.5% (Unchanged Token) | 85% (Unchanged Token) |\n",
    "|------------------|--------------------|----------------------|------------------------|------------------------|\n",
    "| **Input**        | `<MASK>`           | Random token         | Original token         | Original token         |\n",
    "| **Label**        | Original token     | Original token       | Original token         | `<PAD>`                |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffed92b",
   "metadata": {},
   "source": [
    "### NSP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c490fc4",
   "metadata": {},
   "source": [
    "In the NSP task we have 3 important things to be considered:\n",
    "1. **Input**: We take two sentences A and B and the input takes the form $\\underbrace{\\text{[CLS] Sentence A. [SEP]}}_{\\text{Segment 1}} \\underbrace{\\text{Sentence B. [SEP]}}_{\\text{Segment 2}}$;\n",
    "2. **Segment IDs**: to each token of the first sequence we assign a 0 ID, and to each one of the second a 1 ID (CLS, the final punctuation '.', and the first SEP belong to the first sequence)\n",
    "3. **Labels**: to sentence B we assign:\n",
    "    - $y=1$ if the sentence B follows logically from sentence A. In other terms, if the sentence B comes exactly afyer from the sentence A in the dataset;\n",
    "    - $y=0$ if the sentence B does not follow logically from sentence A. In other terms, if the sentence B is sampled casually from dataset;\n",
    "\n",
    "Usually, it is considered also an attention_mask with 1 for real tokens and 0 for padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a377c",
   "metadata": {},
   "source": [
    "# 0) Self-attention and Positional Encoding: PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f44bc0",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c6c8ed",
   "metadata": {
    "id": "e0c6c8ed"
   },
   "source": [
    "### Using OHE (softmax not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e14860",
   "metadata": {
    "id": "06e14860",
    "outputId": "f30a1815-a8d3-43a4-a800-16c16cb359bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': tensor([1., 0., 0., 0., 0.]),\n",
       " 'is': tensor([0., 1., 0., 0., 0.]),\n",
       " 'table': tensor([0., 0., 1., 0., 0.]),\n",
       " 'the': tensor([0., 0., 0., 1., 0.]),\n",
       " 'under': tensor([0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = {\n",
    "    'le': 'the'\n",
    "    , 'chat': 'cat'\n",
    "    , 'est': 'is'\n",
    "    , 'sous': 'under'\n",
    "    , 'la': 'the'\n",
    "    , 'table': 'table'\n",
    "}\n",
    "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
    "vocabulary_out = sorted(list(set(dictionary.values()))) #set uses the unique values\n",
    "\n",
    "def encode_one_hot(vocabulary):\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    one_hot = dict()\n",
    "    LEN = len(vocabulary)\n",
    "\n",
    "    for i, key in enumerate(vocabulary):\n",
    "        one_hot_vector = torch.zeros(LEN)  # Start with a vector of zeros\n",
    "        one_hot_vector[i] = 1  # Set the i-th position to 1 for the current word\n",
    "        one_hot[key] = one_hot_vector  # Map the word to its one-hot encoded vector\n",
    "\n",
    "    return one_hot  # Return the dictionary of words and their one-hot encoded vectors\n",
    "\n",
    "one_hot_in = encode_one_hot(vocabulary_in)\n",
    "one_hot_out = encode_one_hot(vocabulary_out)\n",
    "\n",
    "one_hot_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96882e2d",
   "metadata": {
    "id": "96882e2d"
   },
   "source": [
    "We can now create the fixed matrices K,V to mimic the use of a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb4718c",
   "metadata": {
    "id": "cdb4718c",
    "outputId": "3e9b6b5b-acbe-43d7-9b09-31b5d446933c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 1.])\n",
      "tensor([0.1296, 0.1296, 0.1296, 0.2591, 0.3522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/gw8dmgsd6m11bg8nhrgpd3vr0000gn/T/ipykernel_19754/3583722038.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_refined = nn.functional.softmax(q @ K.T) @ V\n"
     ]
    }
   ],
   "source": [
    "K = torch.stack([one_hot_in[k] for k in dictionary.keys()])\n",
    "V = torch.stack([one_hot_out[k] for k in dictionary.values()])\n",
    "\n",
    "q = one_hot_in['sous']\n",
    "\n",
    "attention = (q @ K.T) @ V\n",
    "\n",
    "print(attention)\n",
    "\n",
    "attention_refined = nn.functional.softmax(q @ K.T) @ V\n",
    "\n",
    "print(attention_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b8c72",
   "metadata": {
    "id": "b25b8c72"
   },
   "source": [
    "Finally we extract the predicted word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316a2d4c",
   "metadata": {
    "id": "316a2d4c",
    "outputId": "5ddd2814-f92f-41c5-bf98-fac36e4ea249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4) tensor(4)\n",
      "under under\n"
     ]
    }
   ],
   "source": [
    "prediction_number = torch.argmax(attention)\n",
    "prediction_number_refined = torch.argmax(attention)\n",
    "\n",
    "print(prediction_number, prediction_number_refined)\n",
    "\n",
    "predicted_word = vocabulary_out[prediction_number]\n",
    "predicted_word_refined = vocabulary_out[prediction_number_refined]\n",
    "\n",
    "print(predicted_word, predicted_word_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af413e21",
   "metadata": {
    "id": "af413e21"
   },
   "source": [
    "### Creating a single-head class from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd4f12",
   "metadata": {},
   "source": [
    "This is a simple class for self-attention without positional encoding. This means that we have an ambedding layer, and then we split with three different layers into Keys, Queries and Values. Recall also that, for the embedding, the input is of vocabulary size, because `nn.Embedding` already selects the rows corresponding to the input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2c172d",
   "metadata": {
    "id": "0b2c172d"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # simple class where d_k = d_v = embed_dim\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "\n",
    "    def forward(self, x): # input is a single sequence tokenized of shape torch.Size([n]), with n = sequence_lenght\n",
    "        embedded_x = self.embedding(x) # torch.Size([n, embed_dim])\n",
    "\n",
    "        K = self.key(embedded_x) # torch.Size([n, d_k = embed_dim])\n",
    "        Q = self.query(embedded_x) # torch.Size([n, d_q = d_v = embed_dim]) \n",
    "        V = self.value(embedded_x) # torch.Size([n, d_v = embed_dim])\n",
    "\n",
    "        attention = nn.functional.softmax(Q @ K.transpose(-2, -1) * K.shape[-1]**-0.5 , dim=-1) @ V # Do a softmax across the last dimension\n",
    "        # NEW SHAPE:  torch.Size([n, embed_dim])\n",
    "        return attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79795f10",
   "metadata": {
    "id": "79795f10"
   },
   "source": [
    "Dataset and standard things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2a46b7",
   "metadata": {
    "id": "3c2a46b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 24, 1],\n",
       " [16, 20, 2],\n",
       " [1, 22, 18, 23, 17],\n",
       " [6, 4, 21, 9, 2],\n",
       " [7, 3, 25, 2],\n",
       " [5, 8, 9, 2],\n",
       " [1, 6, 4, 10, 7, 3, 10, 5, 8],\n",
       " [5, 8, 9, 1],\n",
       " [6, 4, 26, 7, 3, 1],\n",
       " [12, 13, 15, 11, 14],\n",
       " [12, 13, 15, 14, 11]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"), (2,\"Basics of PyTorch\"), (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"), (3,\"Sentiment Analysis using PyTorch\"), (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),(1,\" Machine Translation with NLP \"), (1,\" Named Entity vs Sentiment Analysis  NLP \"),\n",
    "    (3,\"he painted the car red\"), (1,\"he painted the red car\")]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "dataset_tokenized = [text_pipeline(text) for _, text in dataset]\n",
    "\n",
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2981e74",
   "metadata": {
    "id": "c2981e74",
    "outputId": "0162756f-5f95-450d-c6be-45decbf5e627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3568, -0.0575, -0.0071],\n",
      "        [ 0.1397,  0.2781,  0.3226],\n",
      "        [-0.1331,  0.0685,  0.1998],\n",
      "        [-0.0699,  0.0979,  0.2968]], grad_fn=<MmBackward0>)\n",
      "The attention has shape torch.Size([4, 3]), which is 4 x 3.\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 3\n",
    "attention_head = Head(vocab_size = VOCAB_SIZE, embed_dim = EMBED_DIM)\n",
    "\n",
    "single_sequence_tokenized = dataset_tokenized[4]\n",
    "\n",
    "single_sequence_tokenized_torch = torch.tensor(single_sequence_tokenized, dtype = torch.long)\n",
    "\n",
    "attention_single_sequence = attention_head(single_sequence_tokenized_torch)\n",
    "\n",
    "print(attention_single_sequence)\n",
    "\n",
    "print(f\"The attention has shape {attention_single_sequence.shape}, which is {len(single_sequence_tokenized)} x {EMBED_DIM}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70f105",
   "metadata": {
    "id": "6e70f105"
   },
   "source": [
    "### Using the PyTorch multi-head class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ebcec",
   "metadata": {
    "id": "a85ebcec"
   },
   "source": [
    "`nn.MultiheadAttention` is a module in PyTorch that implements the multi-head self-attention mechanism, a key component of the Transformer architecture. This attention mechanism enables the model to focus on different parts of the input sequence simultaneously, capturing various contextual dependencies and improving the model's ability to process complex natural language patterns.\n",
    "\n",
    "The `nn.MultiheadAttention` module has three main inputs: `query`, `key`, and `value` as illustrated below.\n",
    "<p style=\"text-align:center\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/MultiHeadAttention.png\" width=\"300\" alt=\"MultiHead\"/>\n",
    "</p>\n",
    "\n",
    "The multi-head attention mechanism works by first splitting the `query`, `key`, and `value` inputs into multiple \"heads,\" each with its own set of learnable weights. This process allows the model to learn different attention patterns in parallel.\n",
    "\n",
    "The outputs from all heads are concatenated and passed through a linear layer, known as the output projection, to combine the information learned by each head. This final output represents the contextually enriched sequence that can be used in subsequent layers of the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508885eb",
   "metadata": {
    "id": "508885eb",
    "outputId": "0e4aca8d-60df-47b6-9308-28331a213e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should be zero, and it is 0\n",
      "Attention Output Shape is torch.Size([10, 5, 4]), which is 10 x 5 x 4.\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 4\n",
    "num_heads = 2\n",
    "print(\"should be zero, and it is\",embed_dim % num_heads)\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim = embed_dim, num_heads = num_heads, batch_first = False)\n",
    "\n",
    "seq_length = 10 # Sequence length\n",
    "batch_size = 5 # Batch size\n",
    "query = torch.rand((seq_length, batch_size, embed_dim))\n",
    "key = torch.rand((seq_length, batch_size, embed_dim))\n",
    "value = torch.rand((seq_length, batch_size, embed_dim))\n",
    "# Perform multi-head attention\n",
    "attn_output, _= multihead_attn(query, key, value)\n",
    "print(f\"Attention Output Shape is {attn_output.shape}, which is {seq_length} x {batch_size} x {embed_dim}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734c13d",
   "metadata": {
    "id": "a734c13d"
   },
   "source": [
    "The architecture is different even if I set num_heads = 1, for example beacuse there is the concat and the final output layer. For this reason, I can not reproduce the results from the Head class above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18b863",
   "metadata": {
    "id": "3d18b863"
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c43a6",
   "metadata": {
    "id": "df9c43a6"
   },
   "source": [
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0725a13",
   "metadata": {},
   "source": [
    "Recall that the positional encoding comes after the embedding layer, so that the positional encoding dimension is $n$ ( = seq_len ) $\\times d_{\\text{embed}} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f647e",
   "metadata": {},
   "source": [
    "$$ PE(pos,2i)=\\sin\\Big(\\frac{pos}{10000^{2i/h}}\\Big)\\,,\\quad  PE(pos,2i+1)=\\cos\\Big(\\frac{pos}{10000^{2i/h}}\\Big)\\,,\\quad pos=0 ,\\ldots,n-1\\,,\\quad i=0,\\ldots,d_{\\text{embed}}/2 -1\\,,$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a48d7",
   "metadata": {},
   "source": [
    "that we rewrite as in the paper \"attention is all you need\" ($2i = j$):\n",
    "$$ \n",
    "\\text{PE}(pos, j) =\n",
    "\\begin{cases}\n",
    "\\sin\\left(\\frac{pos}{10000^{\\frac{j}{d_{\\text{embed}}}}}\\right) & \\text{if } j \\text{ is even} \\\\\n",
    "\\cos\\left(\\frac{pos}{10000^{\\frac{j-1}{d_{\\text{embed}}}}}\\right) & \\text{if } j \\text{ is odd}\n",
    "\\end{cases}\n",
    "\n",
    "\\,,\\quad pos=0 ,\\ldots,n-1\\,,\\quad j=0,2,\\ldots,d_{\\text{embed}} -2\\,.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a282a",
   "metadata": {},
   "source": [
    "Foe numerical efficency, optimization on GPU and compatibilty with torch tensors, we write the formula as\n",
    "\n",
    "$$ \\frac{pos}{10000^{\\frac{j}{d_{\\text{embed}}}}} = pos \\cdot \\underbrace{\\exp\\left[\\frac{-\\log(10000)}{d_{\\text{model}}} \\cdot j \\right]}_{\\text{term}}\\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a24fc3",
   "metadata": {
    "id": "57a24fc3"
   },
   "source": [
    "If use `nn.TransformerEncoderLayer` in version batch-first (`batch_first = True`), use this: ([batch_size, seq_len, emb_size])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3003e44e",
   "metadata": {
    "id": "3003e44e"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "  \n",
    "        pos = torch.arange(0, max_len, dtype = torch.float).unsqueeze(1) # shape: torch.Size([max_len, 1]), vector from 0,...,max_len-1\n",
    "        term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim) # shape: torch.Size([embed_dim/2]) if embed_dim = even\n",
    "        )                                                                            # if embed_dim = odd torch.Size([(embed_dim+1)/2])\n",
    "                                                                                     # This is torch.Size([embed_dim//2])\n",
    "\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_dim) # initialize a matrix of zero torch.Size([max_len, embed_dim])\n",
    "        pe[:, 0::2] = torch.sin(pos * term) # fill with values\n",
    "        pe[:, 1::2] = torch.cos(pos * term) # fill with values\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape [1, max_len, embed_dim]  for the expected input\n",
    "        self.register_buffer(\"pe\", pe) #this is used since the tensor pe is not filled with learnable parameters\n",
    "\n",
    "        self.dropout = nn.Dropout(p = dropout) # to prevent the model from adapting too much to the absolute position of the tokens\n",
    "\n",
    "    def forward(self, x): # expected x: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1cb170",
   "metadata": {
    "id": "da1cb170"
   },
   "source": [
    "If use  `nn.Transformer`, use this: ([seq_len, batch_size, emb_size])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d55e394d",
   "metadata": {
    "id": "d55e394d"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim)\n",
    "        )\n",
    "\n",
    "  \n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(pos * term) \n",
    "        pe[:, 1::2] = torch.cos(pos * term)  \n",
    "\n",
    "        pe = pe.unsqueeze(1)  # shape: [max_len, 1, embed_dim]\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)  # tensor non learnable salvato nel modello\n",
    "        self.dropout = nn.Dropout(p = dropout)  # dropout per regolarizzazione\n",
    "\n",
    "    def forward(self, x: Tensor): # expected x: [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        x = x + self.pe[:x.size(0), :, :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb1fe7",
   "metadata": {
    "id": "f6bb1fe7"
   },
   "source": [
    "# 1) Encoders for text classification: <span style=\"background-color: yellow\">not Generative AI</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb23234",
   "metadata": {
    "id": "0fb23234"
   },
   "source": [
    "In the first notebook, we used an embedding layer (doing `EmbeddingBag`) and then a linear layer to go to 4 neurons, the number of classes of the AG_NEWS dataset:\n",
    "```\n",
    " def __init__(self, input_dim, hidden_dim, output_dim): # The output dimension is the number of classes in the classification task: 4\n",
    "                                                           # The input is the vocabulary dimension\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(input_dim, hidden_dim, sparse = False)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.init_weights()\n",
    "```\n",
    "This time we will use an encoder, which should enhance the prestation using the attention mechanism!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404907a8",
   "metadata": {
    "id": "404907a8"
   },
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9397a9aa",
   "metadata": {
    "id": "9397a9aa"
   },
   "source": [
    "Taken exactly from notebook 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3cf3902",
   "metadata": {
    "id": "d3cf3902"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "def text_pipeline(x):\n",
    "  return vocab(tokenizer(x))\n",
    "\n",
    "def label_pipeline(x):\n",
    "   return int(x) - 1\n",
    "\n",
    "train_iter, test_iter = iter(AG_NEWS(split=\"train\")), iter(AG_NEWS(split=\"test\"))\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "num_class = len(set([label for (label, text) in train_iter ]))\n",
    "\n",
    "train_iter, test_iter = iter(AG_NEWS(split=\"train\")), iter(AG_NEWS(split=\"test\"))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train, split_valid = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "len(split_train)+len(split_valid)==len(train_dataset)\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text.lower())  # Lowercase conversion for consistency\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae0671",
   "metadata": {
    "id": "4dae0671"
   },
   "source": [
    "The collate function is a bit different. We will use `nn.Embedding` instead of `nn.EmbeddingBag`, so we will not be interested in the offsets but we have to manually manage the padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b884eb33",
   "metadata": {
    "id": "b884eb33"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype = torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first = True) #here the padding is done automatically\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid, batch_size = BATCH_SIZE, shuffle = False, collate_fn = collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size = BATCH_SIZE, shuffle = False, collate_fn = collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad3080",
   "metadata": {
    "id": "e3ad3080"
   },
   "source": [
    "## Accuracy and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85dd3f",
   "metadata": {
    "id": "ba85dd3f"
   },
   "source": [
    "Same predict function as in notebook 1) and similar accuracy as in 1) (the only differences in these is dimensions and the fact that offsets are stripped of):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a49df8f",
   "metadata": {
    "id": "6a49df8f"
   },
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
    "        output = model(text)\n",
    "        return ag_news_label[output.argmax(1).item() + 1]\n",
    "\n",
    "def evaluate(dataloader, model ):\n",
    "    model.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text.to(device))\n",
    "\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba226932",
   "metadata": {
    "id": "ba226932"
   },
   "source": [
    "## Positional encoding (copied from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae94f008",
   "metadata": {
    "id": "ae94f008"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "  \n",
    "        pos = torch.arange(0, max_len, dtype = torch.float).unsqueeze(1) # shape: torch.Size([max_len, 1]), vector from 0,...,max_len-1\n",
    "        term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim) # shape: torch.Size([embed_dim/2]) if embed_dim = even\n",
    "        )                                                                            # if embed_dim = odd torch.Size([(embed_dim+1)/2])\n",
    "                                                                                     # This is torch.Size([embed_dim//2])\n",
    "\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_dim) # initialize a matrix of zero torch.Size([max_len, embed_dim])\n",
    "        pe[:, 0::2] = torch.sin(pos * term) # fill with values\n",
    "        pe[:, 1::2] = torch.cos(pos * term) # fill with values\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape [1, max_len, embed_dim]  for the expected input\n",
    "        self.register_buffer(\"pe\", pe) #this is used since the tensor pe is not filled with learnable parameters\n",
    "\n",
    "        self.dropout = nn.Dropout(p = dropout) # to prevent the model from adapting too much to the absolute position of the tokens\n",
    "\n",
    "    def forward(self, x): # expected x: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20210572",
   "metadata": {
    "id": "20210572"
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1fcf8",
   "metadata": {},
   "source": [
    "This is the class for the encoder. We have an input layer, and its output enters in the positional encoding class (which does the sum internally `x = x + self.pe[:, :x.size(1), :]`). Then the summed output enters in the `nn.TransformerEncoder`, which contains `num_encoder_blocks`blocks of encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0115bc73",
   "metadata": {
    "id": "0115bc73"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, embed_dim = 100, nhead = 5, \n",
    "                 dim_feedforward = 2048, num_encoder_blocks = 6, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            embed_dim = embed_dim,\n",
    "            dropout = dropout,\n",
    "            max_len = max_len,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = embed_dim,\n",
    "            nhead = nhead,\n",
    "            dim_feedforward = dim_feedforward, # number of neurons in th layer after attention of the encoder, which then is re-compressed\n",
    "            dropout = dropout,\n",
    "            batch_first = True  # the output shape will be [batch_size, seq_len, embed_dim]\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers = num_encoder_blocks,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes) #final linear layer for AG classification\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x): # x: [batch_size, seq_len]\n",
    "        \n",
    "        x = self.emb(x) * math.sqrt(self.embed_dim)  # Shape: [batch_size, seq_len, embed_dim]\n",
    "        x = self.pos_encoder(x)                   # Shape: [batch_size, seq_len, embed_dim]\n",
    "        x = self.transformer_encoder(x)           # Shape: [batch_size, seq_len, embed_dim]\n",
    "        x = x.mean(dim = 1)                         # Average over tokens (seq_len)\n",
    "        x = self.classifier(x)                    # Final logits [batch_size, num_classes]\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "467a9e11",
   "metadata": {
    "id": "467a9e11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Encoder                                       [64, 4]                   --\n",
       "├─Embedding: 1-1                              [64, 500, 100]            100\n",
       "├─PositionalEncoding: 1-2                     [64, 500, 100]            --\n",
       "│    └─Dropout: 2-1                           [64, 500, 100]            --\n",
       "├─TransformerEncoder: 1-3                     [64, 500, 100]            --\n",
       "│    └─ModuleList: 2-2                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [64, 500, 100]            452,548\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [64, 500, 100]            452,548\n",
       "│    │    └─TransformerEncoderLayer: 3-3      [64, 500, 100]            452,548\n",
       "│    │    └─TransformerEncoderLayer: 3-4      [64, 500, 100]            452,548\n",
       "│    │    └─TransformerEncoderLayer: 3-5      [64, 500, 100]            452,548\n",
       "│    │    └─TransformerEncoderLayer: 3-6      [64, 500, 100]            452,548\n",
       "├─Linear: 1-4                                 [64, 4]                   404\n",
       "===============================================================================================\n",
       "Total params: 2,715,792\n",
       "Trainable params: 2,715,792\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 158.30\n",
       "===============================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 3632.13\n",
       "Params size (MB): 9.89\n",
       "Estimated Total Size (MB): 3642.28\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBED_DIM = 100\n",
    "vocab_size = len(vocab)\n",
    "model = Encoder(vocab_size = vocab_size, num_classes = 4, embed_dim = EMBED_DIM).to(device)\n",
    "\n",
    "torchinfosummary(model, input_size = (BATCH_SIZE, 500), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b1d00",
   "metadata": {
    "id": "700b1d00"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76eb734",
   "metadata": {
    "id": "a76eb734"
   },
   "source": [
    "Same training (again without offsets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70d7afbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1782 batches, which can also be computed as: '(len(train_dataset) * 0.95)// BATCH_SIZE +1' .\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are a total of {len(train_dataloader)} batches, which can also be computed as: '(len(train_dataset) * 0.95)// BATCH_SIZE +1' .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97d829",
   "metadata": {},
   "source": [
    "It works, but it is commented for time saving (1 epoch is estimated to takes 1h 40m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51012eef",
   "metadata": {
    "colab": {},
    "id": "51012eef",
    "outputId": "60131a6d-9a70-4cd4-a128-6c125f513b1d"
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 1\n",
    "cum_loss_list = []\n",
    "acc_epoch = []\n",
    "acc_old = 0\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1.0, gamma = 0.1) # after 1.0 epochs, the lerning rate will be multiplied by gamma\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     cum_loss = 0.0\n",
    "#     model.train()\n",
    "#     for i, (label, text) in enumerate(tqdm(train_dataloader)):\n",
    "#         optimizer.zero_grad()\n",
    "#         predicted_logits = model(text)\n",
    "#         loss = criterion(predicted_logits, label)\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) #gradient clipping\n",
    "#         optimizer.step()\n",
    "#         cum_loss += loss.item()\n",
    "\n",
    "#     cum_loss_list.append(cum_loss/len(train_dataloader))\n",
    "#     accu_val = evaluate(valid_dataloader, model)\n",
    "#     acc_epoch.append(accu_val)\n",
    "\n",
    "#     if accu_val > acc_old:\n",
    "#       acc_old= accu_val\n",
    "#       torch.save(model.state_dict(), 'my_model.pth')\n",
    "\n",
    "#     #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f7819ec",
   "metadata": {
    "id": "4f7819ec"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(COST,ACC):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot(cum_loss_list, acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52acad84",
   "metadata": {
    "id": "52acad84"
   },
   "source": [
    "# 2a) GPT-like Decoder: <span style=\"background-color: yellow\"> Generative AI</span> for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf0300",
   "metadata": {},
   "source": [
    "## IMDB Dataset and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dacf3bb",
   "metadata": {
    "id": "6dacf3bb"
   },
   "source": [
    "When training language models, it is generally advisable to use general-domain text. However, in this case, we are using the IMDB dataset, which is well-suited for classification tasks. Also, we use IMDB due to its smaller size and compatibility with machines that have limited RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "738564c9",
   "metadata": {
    "executionInfo": {
     "elapsed": 322948,
     "status": "aborted",
     "timestamp": 1752786900780,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "738564c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 12500\n"
     ]
    }
   ],
   "source": [
    "#running time: 1m 36s\n",
    "\n",
    "from torchtext.datasets import IMDB # sentiment analysis dataset with (label, text) pairs\n",
    "\n",
    "train_iter, val_iter = IMDB()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "val_dataset = to_map_style_dataset(val_iter)\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "special_symbols = ['<unk>', '<pad>', '<|endoftext|>']\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "\n",
    "    for _,data_sample in data_iter:\n",
    "        yield  tokenizer(data_sample)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials = special_symbols, special_first = True)\n",
    "vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21019ab1",
   "metadata": {
    "id": "21019ab1"
   },
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22498323",
   "metadata": {
    "id": "22498323",
    "outputId": "9dbe56da-3002-4fca-a694-f5f34cbe24e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')\n",
      "(1, '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.')\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter = IMDB()\n",
    "iterator = iter(train_iter)\n",
    "\n",
    "print(next(iterator))\n",
    "\n",
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174261f9",
   "metadata": {
    "id": "174261f9"
   },
   "source": [
    "The following function gives a string of token (importantly with the white spaces), given a list of indices as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9833fc12",
   "metadata": {
    "executionInfo": {
     "elapsed": 318344,
     "status": "aborted",
     "timestamp": 1752786900792,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "9833fc12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> <pad> <|endoftext|> the . , and'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def index_to_en(ids_list):\n",
    "    string=''\n",
    "    for idx in ids_list:\n",
    "        token = vocab.get_itos()[idx]\n",
    "        string = string + ' ' +token\n",
    "    return string.strip()\n",
    "\n",
    "# index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en]) # done by IBM\n",
    "\n",
    "index_to_en([0,1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef17a3",
   "metadata": {
    "id": "14ef17a3"
   },
   "source": [
    "## Creating the src and tgt sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e3491",
   "metadata": {
    "id": "f54e3491"
   },
   "source": [
    "To train a regressive model, we want to predict the next word. To this end, given a text, we want to take a sentence and then to take the same sentence but shifted of one position. For example, we want to create:\n",
    "\n",
    "```\n",
    "src:  ['around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants']\n",
    "tgt:  ['a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to']\n",
    "```\n",
    "\n",
    "so that we give to the model $x_0$ 'around', and it predicts the logits $z_1$ which must be confronted with 'a ' = $x_1 \\in \\text{tgt}$. <span style=\"background-color: yellow\">Really</span>, $x_i$ are the indices associated to the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cfcb4db",
   "metadata": {
    "executionInfo": {
     "elapsed": 312015,
     "status": "aborted",
     "timestamp": 1752786900805,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "3cfcb4db"
   },
   "outputs": [],
   "source": [
    "def get_sample(block_size, tokenized_text): # the src_ and tgt_sequence will be of block_size lenght\n",
    "                                 \n",
    "    tokenized_text_length = len(tokenized_text)\n",
    "    random_sample_stop = tokenized_text_length - block_size\n",
    "\n",
    "    # Check if a random sample can be taken (if the text is longer than block_size)\n",
    "    if random_sample_stop >= 1:\n",
    "        # Randomly select a starting point for the sample\n",
    "        random_start = torch.randint(low=0, high = random_sample_stop, size=(1,)).item()\n",
    "        # Define the endpoint of the sample\n",
    "        stop = random_start + block_size\n",
    "\n",
    "        # Create the input and target sequences\n",
    "        src_sequence = tokenized_text[random_start:stop]\n",
    "        tgt_sequence= tokenized_text[random_start + 1:stop + 1]\n",
    "\n",
    "    # Handle the case where the text length is exactly equal or less the block size\n",
    "    elif random_sample_stop <= 0:\n",
    "        # Start from the beginning and use the entire text\n",
    "        random_start = 0\n",
    "        stop = tokenized_text_length\n",
    "        src_sequence= tokenized_text[random_start:stop]\n",
    "        tgt_sequence = tokenized_text[random_start + 1:stop]\n",
    "        # Append an empty string to maintain sequence alignment\n",
    "        tgt_sequence.append( '<|endoftext|>')\n",
    "\n",
    "    return vocab(src_sequence), vocab(tgt_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f5adb",
   "metadata": {
    "id": "625f5adb"
   },
   "source": [
    "example of usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3756a9bd",
   "metadata": {
    "executionInfo": {
     "elapsed": 312013,
     "status": "aborted",
     "timestamp": 1752786900806,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "3756a9bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  [36, 239, 244, 15369, 20, 1236, 4504, 12, 93, 12, 2385, 29, 45, 117, 679, 1305, 23, 12, 131, 45, 212, 108, 35669, 6217, 4, 3, 497, 270, 250, 21]\n",
      "tgt:  [239, 244, 15369, 20, 1236, 4504, 12, 93, 12, 2385, 29, 45, 117, 679, 1305, 23, 12, 131, 45, 212, 108, 35669, 6217, 4, 3, 497, 270, 250, 21, 66]\n"
     ]
    }
   ],
   "source": [
    "sentiment, text = next(iterator)\n",
    "text = tokenizer(text)\n",
    "block_size=30\n",
    "src_sequences, tgt_sequence=get_sample(block_size, text)\n",
    "\n",
    "print(\"src: \",src_sequences)\n",
    "print(\"tgt: \",tgt_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822ecf2",
   "metadata": {
    "id": "7822ecf2"
   },
   "source": [
    "## Collate function and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1eee1154",
   "metadata": {
    "executionInfo": {
     "elapsed": 309674,
     "status": "aborted",
     "timestamp": 1752786900807,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "1eee1154"
   },
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 30\n",
    "\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = [], [] #the sum of lists\n",
    "    for sentiment, text in batch:\n",
    "\n",
    "      src_sequence, tgt_sequence = get_sample(BLOCK_SIZE, tokenizer(text)) #src_sequence, tgt_sequence are list of indices\n",
    "\n",
    "      src_sequence = torch.tensor(src_sequence, dtype = torch.int64)\n",
    "      tgt_sequence = torch.tensor(tgt_sequence, dtype = torch.int64)\n",
    "\n",
    "      src_batch.append(src_sequence)\n",
    "      tgt_batch.append(tgt_sequence)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value = PAD_IDX, batch_first = False)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value = PAD_IDX, batch_first = False)\n",
    "\n",
    "    return src_batch.to(device), tgt_batch.to(device)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_iter, val_iter = IMDB()\n",
    "\n",
    "train_dataloader = DataLoader(train_iter, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_batch)\n",
    "# an element of train_loader has shape torch.Size([BLOCK_SIZE, BATCH_SIZE])\n",
    "\n",
    "val_dataloader= DataLoader(val_iter , batch_size = BATCH_SIZE, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b901505",
   "metadata": {
    "id": "7b901505"
   },
   "source": [
    "## Causal Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3dd742",
   "metadata": {},
   "source": [
    "The explanation of the following code is in the Training-Decoder section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a522eea",
   "metadata": {
    "executionInfo": {
     "elapsed": 307585,
     "status": "aborted",
     "timestamp": 1752786900808,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "8a522eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf],\n",
      "        [0., 0., -inf],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(size, device = device):\n",
    "    mask = (torch.triu(torch.ones((size, size), device = device)) == 1).transpose(0, 1).float() #\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "print(generate_square_subsequent_mask(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f23fe7",
   "metadata": {
    "id": "54f23fe7"
   },
   "source": [
    "Next, given a sequence coming from a loader (which has dimension ([BLOCK_SIZE, BATCH_SIZE])), we want to apply the mask to it. Thus we create a src_mask with dimension BLOCK_SIZE x BLOCK_SIZE (that will be passed directly in the model nn.TrasformerDecoderLayer, without adding manually this to the input) and also a src_padding_mask, because our sentences are padded to the right, and the padding must not influence the attention mechanism. For this reason, we will pass to the nn.TrasformerDecoderLayer src_padding_mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9f1cd04",
   "metadata": {
    "executionInfo": {
     "elapsed": 307579,
     "status": "aborted",
     "timestamp": 1752786900809,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "c9f1cd04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_masks(src, device = device):\n",
    "    src_seq_len = src.shape[0] # =BLOCK_SIZE\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(src_seq_len, device)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1) #filled with True and False\n",
    "\n",
    "    return src_mask , src_padding_mask\n",
    "\n",
    "#usage example\n",
    "src, tgt = next(iter(train_dataloader))\n",
    "src_mask, src_padding_mask = create_masks(src)\n",
    "print(src_mask.shape)\n",
    "\n",
    "src_padding_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e763c89",
   "metadata": {
    "id": "8e763c89"
   },
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb42369",
   "metadata": {
    "id": "ffb42369"
   },
   "source": [
    "Second version of the positional encoding (because we used `batch_first = False`) in the `collate_batch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f84e3ad2",
   "metadata": {
    "executionInfo": {
     "elapsed": 304904,
     "status": "aborted",
     "timestamp": 1752786900809,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "f84e3ad2"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim)\n",
    "        )\n",
    "\n",
    "  \n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(pos * term) \n",
    "        pe[:, 1::2] = torch.cos(pos * term)  \n",
    "\n",
    "        pe = pe.unsqueeze(1)  # shape: [max_len, 1, embed_dim]\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)  # tensor non learnable salvato nel modello\n",
    "        self.dropout = nn.Dropout(p = dropout)  # dropout per regolarizzazione\n",
    "\n",
    "    def forward(self, x: Tensor): # expected x: [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        x = x + self.pe[:x.size(0), :, :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269148fd",
   "metadata": {
    "id": "269148fd"
   },
   "source": [
    "## My GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba929dd",
   "metadata": {
    "id": "fba929dd"
   },
   "source": [
    "If confused by the presence of encoder layers, give a look to **SOURCE OF CONFUSION on GPT** in C). Look at the model from the previous 'encoder for AG classification': they are very similar, except from the final layer, which has the vocab size (here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0db8887d",
   "metadata": {
    "executionInfo": {
     "elapsed": 302522,
     "status": "aborted",
     "timestamp": 1752786900812,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "0db8887d"
   },
   "outputs": [],
   "source": [
    "class CustomGPTModel(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, num_heads, num_of_decoder_blocks, max_seq_len = 5000, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        #input layer for embedding and sum with positional encoding\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, dropout = dropout)\n",
    "\n",
    "        # Remaining layers are part of the TransformerDecoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model = embed_size, nhead = num_heads, dropout = dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers = num_of_decoder_blocks)\n",
    "\n",
    "        # output layer for prediction\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "      for p in self.parameters():\n",
    "          if p.dim() > 1:\n",
    "              nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    def forward(self, x, src_mask = None, src_padding_mask = None):\n",
    "\n",
    "        if src_mask is None and src_padding_mask is None:\n",
    "\n",
    "            src_mask, src_padding_mask = create_masks(x)\n",
    "\n",
    "        seq_length = x.size(0)\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        x = self.embed(x)* math.sqrt(self.embed_size) #src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        output = self.transformer_encoder(x, mask = src_mask, src_key_padding_mask = src_padding_mask)\n",
    "        logits = self.lm_head(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55314268",
   "metadata": {
    "executionInfo": {
     "elapsed": 302522,
     "status": "aborted",
     "timestamp": 1752786900813,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "55314268"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "num_of_decoder_blocks = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "\n",
    "model = CustomGPTModel(embed_size = emsize, num_heads = nhead,\n",
    "                        num_of_decoder_blocks = num_of_decoder_blocks, vocab_size = ntokens,\n",
    "                        dropout = dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86783e5",
   "metadata": {
    "id": "c86783e5"
   },
   "source": [
    "## (proof of) Text generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bdd4c",
   "metadata": {
    "id": "bb8bdd4c"
   },
   "source": [
    "The text generation needs a starting point, which is called prompt. This prompt should be tokenized, should not be None and should not be too long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0c27d63",
   "metadata": {
    "executionInfo": {
     "elapsed": 298754,
     "status": "aborted",
     "timestamp": 1752786900814,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "a0c27d63"
   },
   "outputs": [],
   "source": [
    "def indicized_prompt(prompt, block_size = BLOCK_SIZE):\n",
    "    # Handle None prompt\n",
    "    while prompt is None:\n",
    "        prompt = input(\"Sorry, prompt cannot be empty. Please enter a valid prompt: \")\n",
    "\n",
    "    tokens = tokenizer(prompt)\n",
    "    number_of_tokens = len(tokens)\n",
    "\n",
    "    # Handle long prompts\n",
    "    if number_of_tokens > block_size:\n",
    "        tokens = tokens[-block_size:]  # Keep last block_size characters\n",
    "\n",
    "    prompt_indices = vocab(tokens)\n",
    "\n",
    "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
    "    return prompt_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4bd09",
   "metadata": {
    "id": "d0e4bd09"
   },
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6eae958",
   "metadata": {
    "executionInfo": {
     "elapsed": 298751,
     "status": "aborted",
     "timestamp": 1752786900815,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "f6eae958"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gave'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() #to deactivate dropout\n",
    "prompt_encoded = indicized_prompt(\"This is a prompt to get model generate next words.\").to(device) #11 tokens\n",
    "logits = model(prompt_encoded, src_mask = None).to(device)\n",
    "logits.shape #torch.Size([11, 1, 100685])\n",
    "logits = logits.transpose(0, 1) #torch.Size([1, 11, 100685])\n",
    "logits.shape\n",
    "logit_preiction =logits[:,-1]\n",
    "_, next_word_index = torch.max(logit_preiction, dim=1)\n",
    "next_word_index\n",
    "\n",
    "index_to_en(next_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03f9b5",
   "metadata": {
    "id": "de03f9b5"
   },
   "source": [
    "Then we can define a function that, given the indicized_prompt, do generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2db8727d",
   "metadata": {
    "executionInfo": {
     "elapsed": 298751,
     "status": "aborted",
     "timestamp": 1752786900816,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "2db8727d"
   },
   "outputs": [],
   "source": [
    "def generate(model, prompt=None, max_new_tokens = 500, block_size = BLOCK_SIZE, vocab = vocab, tokenizer = tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    prompt_encoded = indicized_prompt(prompt).to(device)\n",
    "    tokens = []\n",
    "\n",
    "    # Generate new tokens up to max_new_tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Decode the encoded prompt using the model's decoder\n",
    "        logits = model(prompt_encoded,src_mask=None,src_padding_mask=None)\n",
    "\n",
    "        # Transpose the logits to bring the sequence length to the first dimension\n",
    "        logits = logits.transpose(0, 1)\n",
    "\n",
    "        # Select the logits of the last token in the sequence\n",
    "        logit_prediction = logits[:, -1]\n",
    "\n",
    "        # Choose the most probable next token from the logits(greedy decoding)\n",
    "        next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
    "\n",
    "        # If the next token is the end-of-sequence (EOS) token, stop generation\n",
    "        if next_token_encoded.item() == EOS_IDX:\n",
    "            break\n",
    "\n",
    "        # Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
    "        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
    "\n",
    "        # Convert the next token index to a token string using the vocabulary\n",
    "        # Move the tensor back to CPU for vocab lookup if needed\n",
    "        token_id = next_token_encoded.to('cpu').item()\n",
    "        tokens.append(vocab.get_itos()[token_id])\n",
    "\n",
    "    # Join the generated tokens into a single string and return\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84d2f0",
   "metadata": {
    "id": "0a84d2f0"
   },
   "source": [
    "The generation, of course, is awful because we did not train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "581f0822",
   "metadata": {
    "executionInfo": {
     "elapsed": 298751,
     "status": "aborted",
     "timestamp": 1752786900817,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "581f0822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gave gave gave gave gave gave gave eschelons eschelons eschelons computing computing computing computing eschelons eschelons eschelons captioned computing wave wave eschelons eschelons dive-bombing dive-bombing zang noodle zang noodle zang'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model,prompt=\"this is the beginning of\",max_new_tokens = 30,vocab = vocab,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af6c82",
   "metadata": {
    "id": "91af6c82"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1caf36f",
   "metadata": {
    "executionInfo": {
     "elapsed": 296196,
     "status": "aborted",
     "timestamp": 1752786900819,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "c1caf36f"
   },
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for src,tgt in eval_data:\n",
    "            tgt = tgt.to(device)\n",
    "            #seq_len = src.size(0)\n",
    "            logits = model(src)\n",
    "            total_loss +=  loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1)).item()\n",
    "    return total_loss / (len(list(eval_data)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76236c48",
   "metadata": {
    "executionInfo": {
     "elapsed": 296197,
     "status": "aborted",
     "timestamp": 1752786900820,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "76236c48"
   },
   "outputs": [],
   "source": [
    "#evaluate(model,val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768d7da",
   "metadata": {
    "id": "4768d7da"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64caf9e3",
   "metadata": {
    "id": "64caf9e3"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)\n",
    "\n",
    "def train(model: nn.Module,train_data) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 10000\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(list(train_data)) //BLOCK_SIZE\n",
    "    for batch,srctgt in enumerate(tqdm(train_data)):\n",
    "        src= srctgt[0]\n",
    "        tgt= srctgt[1]\n",
    "        logits = model(src,src_mask=None)\n",
    "        logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "        loss = loss_fn(logits_flat, tgt.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch % log_interval == 0 and batch > 0) or batch==42060:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            #cur_loss = total_loss / log_interval\n",
    "            cur_loss = total_loss / batch\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch//BLOCK_SIZE:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            start_time = time.time()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72abe5",
   "metadata": {},
   "source": [
    "It works, but it is commented for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1517fc6d",
   "metadata": {
    "id": "1517fc6d",
    "outputId": "53eeb7df-64b3-4169-ac5d-fdb082685205"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 1\n",
    "Train_losses= []\n",
    "Val_losses = []\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train_loss = train(model,train_dataloader)\n",
    "#     val_loss = evaluate(model, val_dataloader)\n",
    "#     val_ppl = math.exp(val_loss)\n",
    "#     Train_losses.append(train_loss)\n",
    "#     Val_losses.append(val_loss)\n",
    "\n",
    "#     elapsed = time.time() - epoch_start_time\n",
    "#     print('-' * 89)\n",
    "#     print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "#         f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "#     print('-' * 89)\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), 'model_best_val_loss.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9966c057",
   "metadata": {
    "id": "9966c057"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the number of epochs (assuming the lengths of train_losses and val_losses are equal)\n",
    "num_epochs = len(Train_losses)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the training losses\n",
    "ax.plot(range(num_epochs), Train_losses, label='Training Loss', color='blue')\n",
    "\n",
    "# Plot the validation losses\n",
    "ax.plot(range(num_epochs), Val_losses, label='Validation Loss', color='orange')\n",
    "\n",
    "# Set the x-axis label\n",
    "ax.set_xlabel('Epoch')\n",
    "\n",
    "# Set the y-axis label\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "# Set the title of the plot\n",
    "ax.set_title('Training and Validation Losses')\n",
    "\n",
    "# Add a legend to the plot\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2305cd",
   "metadata": {},
   "source": [
    "# 2b) MLM+NSP pre-training of a BERT-like Encoder, with PyTorch\n",
    "\n",
    "Here we do the pre-training from scratch. Later, we will do the same but we take a model from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5d0d4",
   "metadata": {},
   "source": [
    "## IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4186a6f",
   "metadata": {},
   "source": [
    "We start with the usual things, as in 2a), but with more special symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9c2afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 125092\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "all_data_iter = chain(train_iter, test_iter)\n",
    "\n",
    "PAD_IDX, CLS_IDX, SEP_IDX,  MASK_IDX, UNK_IDX = 0, 1, 2, 3, 4\n",
    "special_symbols = ['[PAD]','[CLS]', '[SEP]','[MASK]','[UNK]']\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "\n",
    "    for label, data_sample in data_iter:\n",
    "        yield  tokenizer(data_sample)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(all_data_iter), specials = special_symbols, special_first = True) #build vocab from all data\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print('Size of vocabulary:', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "524779cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'would', 'put', 'this', 'at', 'the', 'top', 'of', 'my', 'list', 'of', 'films', 'in', 'the', 'category', 'of', 'unwatchable', 'trash', '!', 'there']\n"
     ]
    }
   ],
   "source": [
    "def index_to_en(ids_list): #index_to_english\n",
    "    string=''\n",
    "    for idx in ids_list:\n",
    "        token = vocab.get_itos()[idx]\n",
    "        string = string + ' ' +token\n",
    "    return string.strip()\n",
    "\n",
    "all_data_iter = chain(train_iter, test_iter)\n",
    "\n",
    "fifth_item_tokens = next(islice(yield_tokens(all_data_iter), 5, None))\n",
    "print(fifth_item_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86822607",
   "metadata": {},
   "source": [
    "Then we want to prepare data for MLM and NSP. We define a MLM and a NSP function, which at the end will be unified. Notice that MLM is used also in NSP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6d3c5",
   "metadata": {},
   "source": [
    "## MLM (Masked Language Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01201e1c",
   "metadata": {},
   "source": [
    "First we should have a method to compute random numbers. The `bernoulli_true_false` below does the game, but it is too complicated for a simple task. Moreover, as also `random.random()` from Python, is not compatible with GPU and it does not enter in the backprop scheme of PyTorch. For these reason, it is preferable to use `torch.rand()` or `torch.bernoulli()` from PyTorch. Attention!! Not use `torch.randn()`, which even if similar in the name, generates a normal distribution and not a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c25ba7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bernoulli_true_false by IBM, we will not use it\n",
    "\n",
    "def bernoulli_true_false(probability):\n",
    "    # Create a Bernoulli distribution with probability p\n",
    "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([probability]))\n",
    "    # Sample from this distribution and convert 1 to True and 0 to False\n",
    "    return bernoulli_dist.sample().item() == 1\n",
    "\n",
    "bernoulli_true_false(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6032ab",
   "metadata": {},
   "source": [
    "The following functions decide whether each token in a sequence should be masked, left unchanged, or replaced with a random token. This process is essential for training the model to predict masked words based on their context.\n",
    "\n",
    "So we prepare a function for masking a single token, and then apply to a whole tokenized sequence in the subsequent function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f37934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking done by IBM, with bernoulli_true_false and weird probabilities\n",
    "\n",
    "def Masking(token):\n",
    "    \n",
    "    mask = bernoulli_true_false(0.2) # Decide whether to mask this token (20% chance); this is True or False\n",
    "\n",
    "    if mask is False: # If mask is False, return with '[PAD]' label\n",
    "        token_ = token\n",
    "        mask_label = '[PAD]'\n",
    "\n",
    "    random_opp = bernoulli_true_false(0.5)\n",
    "    random_swich = bernoulli_true_false(0.5)\n",
    "\n",
    "    if mask is True and random_opp is True and random_swich is True: # Replace the token with '[MASK]' and set label to a random token\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
    "    \n",
    "    elif mask is True and random_opp is True and random_swich is False: # Leave the token unchanged and set label to the same token\n",
    "        token_ = token\n",
    "        mask_label = token\n",
    "\n",
    "    else: # Replace the token with '[MASK]' and set label to the original token\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = token\n",
    "\n",
    "    return token_, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e82dcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This respects the original BERT probabilities\n",
    "\n",
    "def BERT_Masking(original_token): # it returns a couple of input_token, label_token\n",
    "   \n",
    "    if torch.rand(1).item() >= 0.15:  # Decide if this token should be masked (15% chance)\n",
    "        input_token = original_token\n",
    "        label_token = '[PAD]'\n",
    "        return input_token, label_token\n",
    "\n",
    "    rand = torch.rand(1).item() # Choose the masking strategy\n",
    "\n",
    "    if rand < 0.8:  # 80% of the time → [MASK] in input, original token in label\n",
    "        input_token = '[MASK]'\n",
    "        label_token = original_token\n",
    "        return input_token, label_token\n",
    "    \n",
    "    elif rand < 0.9: # 10% of the time → original token in input, original token in label\n",
    "        input_token = original_token\n",
    "        label_token = original_token\n",
    "        return input_token, label_token\n",
    "    \n",
    "    else: # 10% of the time → random token in input, original token in label\n",
    "        \n",
    "        random_index = torch.randint(0, VOCAB_SIZE, (1,))\n",
    "        random_token = index_to_en(random_index)\n",
    "        input_token = random_token\n",
    "        label_token = original_token\n",
    "        return input_token, label_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a50a3",
   "metadata": {},
   "source": [
    "Apply the masking to a whole sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "719e7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mlm(tokenized_sequence, include_raw_tokens = False):\n",
    "    raw_tokens_list = []  # List to store raw tokens if needed\n",
    "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
    "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
    "    current_raw_tokens = []\n",
    "    current_bert_input = []\n",
    "    current_bert_label = []\n",
    "\n",
    "    for token in tokenized_sequence:\n",
    "        masked_token, mask_label = BERT_Masking(token)\n",
    "\n",
    "        current_bert_input.append(masked_token)\n",
    "        current_bert_label.append(mask_label)\n",
    "\n",
    "        if include_raw_tokens is True:\n",
    "            current_raw_tokens.append(token)\n",
    "\n",
    "        if token in ['.', '?', '!']: # Check if the token is a sentence delimiter (., ?, !)\n",
    "            \n",
    "            if len(current_bert_input) > 2: # If current sentence has more than two tokens, consider it a valid sentence\n",
    "                bert_input.append(current_bert_input)\n",
    "                bert_label.append(current_bert_label)\n",
    "                \n",
    "                if include_raw_tokens: # If including raw tokens, add the current list of raw tokens to the raw tokens list\n",
    "                    raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "                # Reset the lists for the next sentence\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "\n",
    "            else: # If the current sentence is too short, discard it and reset lists\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "\n",
    "    if current_bert_input: # Add any remaining tokens as a sentence if there are any\n",
    "        bert_input.append(current_bert_input)\n",
    "        bert_label.append(current_bert_label)\n",
    "        if include_raw_tokens:\n",
    "            raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "    # Return the prepared lists for BERT's MLM training\n",
    "    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b57d9",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e142f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: [['the', 'sun', 'sets', 'behind', 'the', 'distant', 'mountains']]\n",
      "bert_input is: [['[MASK]', 'sun', 'sets', 'behind', 'the', 'distant', 'mountains']]\n",
      "bert_label is: [['the', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n"
     ]
    }
   ],
   "source": [
    "original_input = \"The sun sets behind the distant mountains\"\n",
    "tokenized_sequence = tokenizer(original_input)\n",
    "\n",
    "bert_input, bert_label, raw_tokens_list = prepare_for_mlm(tokenized_sequence, include_raw_tokens = True)\n",
    "\n",
    "print('raw:',raw_tokens_list)\n",
    "print('bert_input is:',bert_input)\n",
    "print('bert_label is:',bert_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda79f8",
   "metadata": {},
   "source": [
    "## NSP (Next Sequence Predictions) + MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f816733",
   "metadata": {},
   "source": [
    "The following is an intermediate step. At the moment, for each sentence we have prepared the masked sentence and the masked label using the `prepare_for_mlm` function. Then, given a list of sentences, we want to create the mlm things and thn split both in the nsp sense (that is, add the nsp label 0/1 to sentence B and also add < CLS > and < PAD >)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06008ba7",
   "metadata": {},
   "source": [
    "The input of the following function should be the couple (input_mlm_sentences, input_mlm_labels), obtained from `prepare_for_mlm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ab07828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_nsp(input_mlm_sentences, input_mlm_labels): # input_mlm_sentences = list of tokenized sentences\n",
    "                                                           # input_mlm_labels = corresponding label masked list \n",
    "                                                           #                       will be the result of prepare_for_mlm function\n",
    "    if len(input_mlm_sentences) < 2:\n",
    "       raise ValueError(\"Must have at least two sentences.\")\n",
    "\n",
    "    # Verify that both input lists are of the same length and have a sufficient number of sentences\n",
    "    if len(input_mlm_sentences) != len(input_mlm_labels):\n",
    "        raise ValueError(\"Both lists must have the same number of items.\")\n",
    "\n",
    "    bert_input = []\n",
    "    mlm_label = []\n",
    "    nsp_label = []\n",
    "    available_indices = list(range(len(input_mlm_sentences)))\n",
    "\n",
    "    while len(available_indices) >= 2:\n",
    "        if random.random() < 0.5:\n",
    "            # Choose two consecutive sentences to simulate the 'next sentence' scenario\n",
    "            index = random.choice(available_indices[:-1])  # Exclude the last index\n",
    "            # append list and add  '[CLS]' and  '[SEP]' tokens\n",
    "            bert_input.append([['[CLS]']+input_mlm_sentences[index]+ ['[SEP]'],input_mlm_sentences[index + 1]+ ['[SEP]']])\n",
    "            mlm_label.append([['[PAD]']+input_mlm_labels[index]+['[PAD]'], input_mlm_labels[index + 1]+ ['[PAD]']])\n",
    "            nsp_label.append(1)  # Label 1 indicates these sentences are consecutive\n",
    "\n",
    "            # Remove the used indices\n",
    "            available_indices.remove(index)\n",
    "            if index + 1 in available_indices:\n",
    "                available_indices.remove(index + 1)\n",
    "        else:\n",
    "            # Choose two random distinct sentences to simulate the 'not next sentence' scenario\n",
    "            indices = random.sample(available_indices, 2)\n",
    "            bert_input.append([['[CLS]']+input_mlm_sentences[indices[0]]+['[SEP]'],input_mlm_sentences[indices[1]]+ ['[SEP]']])\n",
    "            mlm_label.append([['[PAD]']+input_mlm_labels[indices[0]]+['[PAD]'], input_mlm_labels[indices[1]]+['[PAD]']])\n",
    "            nsp_label.append(0)  # Label 0 indicates these sentences are not consecutive\n",
    "\n",
    "            # Remove the used indices\n",
    "            available_indices.remove(indices[0])\n",
    "            available_indices.remove(indices[1])\n",
    "            \n",
    "    return bert_input, mlm_label, nsp_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32c577",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e892f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Input:\n",
      "[['[CLS]', ['i', 'love', 'apples'], '[SEP]'], [['she', 'enjoys', 'reading', 'books'], '[SEP]']]\n",
      "MLM Label:\n",
      "[['[PAD]', ['[PAD]', '[PAD]', '[PAD]'], '[PAD]'], [['[PAD]', '[PAD]', '[PAD]', '[PAD]'], '[PAD]']]\n",
      "NSP Label (is next):  [1]\n"
     ]
    }
   ],
   "source": [
    "original_sentences = [[\"i\", \"love\", \"apples\"], [\"she\", \"enjoys\", \"reading\", \"books\"], [\"he\", \"likes\", \"playing\", \"guitar\", \"a\",\"lot\"]]\n",
    "\n",
    "input_mlm_sentences = []     # contains masked input\n",
    "input_mlm_labels = []           # contains corresponding MLM labels\n",
    "\n",
    "for sentence in original_sentences:\n",
    "    masked, labels = prepare_for_mlm(sentence)\n",
    "    input_mlm_sentences.append(masked)\n",
    "    input_mlm_labels.append(labels)\n",
    "\n",
    "bert_input, mlm_label, nsp_label = process_for_nsp(input_mlm_sentences, input_mlm_labels)\n",
    "\n",
    "print(\"BERT Input:\")\n",
    "for pair in bert_input:\n",
    "    print(pair)\n",
    "print(\"MLM Label:\")\n",
    "for pair in mlm_label:\n",
    "    print(pair)\n",
    "print(\"NSP Label (is next): \", nsp_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5f96b",
   "metadata": {},
   "source": [
    "What is still missing? It is missing the padding, the segmentation ( segment id 0 to segment A and 1 to segment B) and the conversion in indices using vocabulary for being used by BERT.\n",
    "\n",
    "The following function takes as input the ouput of the `process_for_nsp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac88cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_final_inputs(bert_input, mlm_label, nsp_label, to_tensor=True):\n",
    "    \"\"\"\n",
    "    Prepara input_ids, mlm_labels, segment_ids, nsp_labels per l'input a BERT.\n",
    "    \"\"\"\n",
    "\n",
    "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair\n",
    "\n",
    "    def flatten(l):\n",
    "        return [item for sublist in l for item in sublist]\n",
    "\n",
    "    def tokens_to_index(tokens):\n",
    "        return [vocab[token] if isinstance(token, str) else vocab[str(token)] for token in tokens]\n",
    "\n",
    "    input_ids_final = []\n",
    "    mlm_labels_final = []\n",
    "    segment_ids_final = []\n",
    "    nsp_labels_final = []\n",
    "\n",
    "    for input_pair, label_pair, is_next in zip(bert_input, mlm_label, nsp_label):\n",
    "        # Segment IDs: 0 per frase A, 1 per frase B\n",
    "        segment_ids = [[0] * len(input_pair[0]), [1] * len(input_pair[1])]\n",
    "\n",
    "        # Padding\n",
    "        input_padded = zero_pad_list_pair(input_pair)\n",
    "        label_padded = zero_pad_list_pair(label_pair)\n",
    "        segment_padded = zero_pad_list_pair(segment_ids, pad=0)\n",
    "\n",
    "        # Flatten\n",
    "        input_flat = flatten(input_padded)\n",
    "        label_flat = flatten(label_padded)\n",
    "        segment_flat = flatten(segment_padded)\n",
    "\n",
    "        # Conversione e append\n",
    "        if to_tensor:\n",
    "            input_ids_final.append(torch.tensor(tokens_to_index(input_flat), dtype=torch.int64))\n",
    "            mlm_labels_final.append(torch.tensor(tokens_to_index(label_flat), dtype=torch.int64))\n",
    "            segment_ids_final.append(torch.tensor(segment_flat, dtype=torch.int64))\n",
    "            nsp_labels_final.append(torch.tensor(is_next, dtype=torch.int64))\n",
    "        else:\n",
    "            input_ids_final.append(tokens_to_index(input_flat))\n",
    "            mlm_labels_final.append(tokens_to_index(label_flat))\n",
    "            segment_ids_final.append(segment_flat)\n",
    "            nsp_labels_final.append(is_next)\n",
    "\n",
    "    return input_ids_final, mlm_labels_final, segment_ids_final, nsp_labels_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddcad9",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56f893f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\t\t [[['[CLS]', ['i', 'love', 'apples'], '[SEP]'], [['[MASK]', 'enjoys', 'reading', 'books'], '[SEP]']]] \n",
      "inputs_final:\t [tensor([1, 4, 2, 4, 2, 0])] \n",
      "bert labels final:\t [tensor([0, 4, 0, 4, 0, 0])] \n",
      "segment labels final:\t [tensor([0, 0, 0, 1, 1, 0])] \n",
      "is nexts final:\t [tensor(1)]\n"
     ]
    }
   ],
   "source": [
    "original_sentences = [[\"i\", \"love\", \"apples\"], [\"she\", \"enjoys\", \"reading\", \"books\"], [\"he\", \"likes\", \"playing\", \"guitar\", \"a\",\"lot\"]]\n",
    "\n",
    "input_mlm_sentences = []     # contains masked input\n",
    "input_mlm_labels = []           # contains corresponding MLM labels\n",
    "\n",
    "for sentence in original_sentences:\n",
    "    masked, labels = prepare_for_mlm(sentence)\n",
    "    input_mlm_sentences.append(masked)\n",
    "    input_mlm_labels.append(labels)\n",
    "\n",
    "bert_input, mlm_label, nsp_label = process_for_nsp(input_mlm_sentences, input_mlm_labels)\n",
    "\n",
    "bert_inputs_final, mlm_labels_final, segment_labels_final, nsp_labels_final = prepare_bert_final_inputs(bert_input, \n",
    "                                                                                                        mlm_label, \n",
    "                                                                                                        nsp_label, \n",
    "                                                                                                        to_tensor = True)\n",
    "\n",
    "\n",
    "\n",
    "torch.set_printoptions(linewidth=10000)# this assures that whole output is printed in one line\n",
    "print(\"input:\\t\\t\",bert_input,\"\\ninputs_final:\\t\",bert_inputs_final,\"\\nbert labels final:\\t\",mlm_labels_final,\"\\nsegment labels final:\\t\",segment_labels_final,\"\\nis nexts final:\\t\",nsp_labels_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0cf02",
   "metadata": {},
   "source": [
    "## BERT Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c31a18",
   "metadata": {},
   "source": [
    "Then we create a csv final which contains all this. This will be the dataset for training the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa30ae3",
   "metadata": {},
   "source": [
    "This is commented because it is very very long. In the next section, we will load the already-done dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00a91bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = 'train_bert_data_new.csv'\n",
    "\n",
    "# with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     csv_writer = csv.writer(file)\n",
    "#     csv_writer.writerow(['Original Text', 'Input IDs', 'MLM Labels', 'Segment IDs', 'Is Next'])\n",
    "\n",
    "#     # Wrap train_iter with tqdm for a progress bar\n",
    "#     for n, (_, sample) in enumerate(tqdm(train_iter, desc=\"Processing samples\")):\n",
    "#         # Tokenize the sample input\n",
    "#         tokens = tokenizer(sample)\n",
    "\n",
    "#         # Create MLM inputs and labels\n",
    "#         mlm_inputs, mlm_labels = prepare_for_mlm(tokens, include_raw_tokens=False)\n",
    "#         if len(mlm_inputs) < 2:\n",
    "#             continue  # Skip short samples\n",
    "\n",
    "#         # Create NSP input pairs and labels\n",
    "#         bert_input, mlm_label, nsp_label = process_for_nsp(mlm_inputs, mlm_labels)\n",
    "\n",
    "#         # Add padding, create segment labels, and convert to indices/tensors\n",
    "#         input_ids_final, mlm_labels_final, segment_ids_final, nsp_labels_final = prepare_bert_final_inputs(\n",
    "#             bert_input, mlm_label, nsp_label, to_tensor=True\n",
    "#         )\n",
    "\n",
    "#         # Convert tensors to list, format as JSON strings, and write to CSV\n",
    "#         for input_ids, mlm_ids, segment_ids, is_next in zip(\n",
    "#             input_ids_final, mlm_labels_final, segment_ids_final, nsp_labels_final\n",
    "#         ):\n",
    "#             input_ids_str = json.dumps(input_ids.tolist())\n",
    "#             mlm_labels_str = json.dumps(mlm_ids.tolist())\n",
    "#             segment_ids_str = ','.join(map(str, segment_ids.tolist()))\n",
    "#             is_next_label = is_next.item() if isinstance(is_next, torch.Tensor) else int(is_next)\n",
    "\n",
    "#             csv_writer.writerow([sample, input_ids_str, mlm_labels_str, segment_ids_str, is_next_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bfa08",
   "metadata": {},
   "source": [
    "## BERT Dataset loading and Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f42e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-03 16:18:08--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
      "Risoluzione di cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
      "Connessione a cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 88958506 (85M) [application/zip]\n",
      "Salvataggio in: «BERT_dataset.zip»\n",
      "\n",
      "BERT_dataset.zip    100%[===================>]  84.84M   638KB/s    in 80s     \n",
      "\n",
      "2025-08-03 16:19:30 (1.06 MB/s) - «BERT_dataset.zip» salvato [88958506/88958506]\n",
      "\n",
      "Archive:  BERT_dataset.zip\n",
      "   creating: /Users/alex/Desktop/programmazione/notebooks/My notebooks/IBM Generative AI with LLMs/bert_dataset\n",
      "  inflating: bert_dataset/.DS_Store  \n",
      "  inflating: bert_dataset/bert_train_data.csv  \n",
      "  inflating: bert_dataset/bert_test_data_sampled.csv  \n",
      "  inflating: bert_dataset/bert_test_data.csv  \n",
      "  inflating: bert_dataset/bert_train_data_sampled.csv  \n"
     ]
    }
   ],
   "source": [
    "# 1m28s\n",
    "\n",
    "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
    "!unzip -o BERT_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba1ed3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCSVDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.data = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        try:\n",
    "            # Carica i dati dal CSV con nomi coerenti con la scrittura\n",
    "            input_ids = torch.tensor(json.loads(row['Input IDs']), dtype=torch.long)\n",
    "            mlm_labels = torch.tensor(json.loads(row['MLM Labels']), dtype=torch.long)\n",
    "            segment_ids = torch.tensor([int(x) for x in row['Segment IDs'].split(',')], dtype=torch.long)\n",
    "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
    "            nsp_label = is_next\n",
    "            original_text = row['Original Text']\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
    "            print(\"Input IDs:\", row['Input IDs'])\n",
    "            print(\"MLM Labels:\", row['MLM Labels'])\n",
    "            return None  # Skips this sample\n",
    "\n",
    "        # Tokenizzazione del testo originale (opzionale)\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            original_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        original_input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "\n",
    "        return (\n",
    "            input_ids,          # Input tokens (with [CLS], [SEP], etc.)\n",
    "            mlm_labels,         # MLM target labels\n",
    "            segment_ids,        # Segment IDs (0/1)\n",
    "            nsp_label,            # NSP label\n",
    "            original_input_ids, # Optional: tokenized original sentence\n",
    "            attention_mask,     # Optional: attention mask for original input\n",
    "            original_text       # Raw input text\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca2f43",
   "metadata": {},
   "source": [
    "## Collate function and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f4ca7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = 0\n",
    "def collate_batch(batch):\n",
    "\n",
    "   \n",
    "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]\n",
    "\n",
    "    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(is_next)\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        original_text_battch.append(original_text)\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
    "test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2278a",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63119b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Define the PositionalEncoding class as a PyTorch module for adding positional information to token embeddings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        # Apply the positional encodings to the input token embeddings\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bf6ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding (nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
    "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
    "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels=False):\n",
    "        my_embeddings=self.token_embedding(bert_inputs)\n",
    "        if self.training:\n",
    "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
    "        else:\n",
    "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d4b7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        vocab_size: The size of the vocabulary.\n",
    "        d_model: The size of the embeddings (hidden size).\n",
    "        n_layers: The number of Transformer layers.\n",
    "        heads: The number of attention heads in each Transformer layer.\n",
    "        dropout: The dropout rate applied to embeddings and Transformer layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # Embedding layer that combines token embeddings and segment embeddings\n",
    "        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)\n",
    "\n",
    "        # Transformer Encoder layers\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Linear layer for Next Sentence Prediction\n",
    "        self.nextsentenceprediction = nn.Linear(d_model, 2)\n",
    "\n",
    "        # Linear layer for Masked Language Modeling\n",
    "        self.masked_language = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels):\n",
    "        \"\"\"\n",
    "        bert_inputs: Input tokens.\n",
    "        segment_labels: Segment IDs for distinguishing different segments in the input.\n",
    "        mask: Attention mask to prevent attention to padding tokens.\n",
    "\n",
    "        return: Predictions for next sentence task and masked language modeling task.\n",
    "        \"\"\"\n",
    "\n",
    "        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
    "        # Generate embeddings from input tokens and segment labels\n",
    "        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)\n",
    "\n",
    "        # Pass embeddings through the Transformer encoder\n",
    "        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask=padding_mask)\n",
    "\n",
    "\n",
    "        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
    "        \n",
    "\n",
    "        # Masked Language Modeling: Predict all tokens in the sequence\n",
    "        masked_language = self.masked_language(transformer_encoder_output)\n",
    "\n",
    "        return  next_sentence_prediction, masked_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c046c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 147161  # Replace VOCAB_SIZE with your vocabulary size\n",
    "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
    "n_layers = 2  # Number of Transformer layers\n",
    "initial_heads = 12 # Initial number of attention heads\n",
    "initial_heads = 2\n",
    "# Ensure the number of heads is a factor of the embedding dimension\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Create an instance of the BERT model\n",
    "model = BERT(vocab_size, d_model, n_layers, heads, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc0a5d",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e3b4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)# The loss function must ignore PAD tokens and only calculates loss for the masked tokens\n",
    "loss_fn_nsp = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b9f10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader=test_dataloader, model=model, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):\n",
    "    model.eval()  # Turn off dropout and other training-specific behaviors\n",
    "\n",
    "    total_loss = 0\n",
    "    total_next_sentence_loss = 0\n",
    "    total_mask_loss = 0\n",
    "    total_batches = 0\n",
    "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "        for batch in dataloader:\n",
    "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "            # Forward pass\n",
    "            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
    "\n",
    "            # Calculate loss for next sentence prediction\n",
    "            # Ensure is_nexts is of the correct shape for CrossEntropyLoss\n",
    "            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))\n",
    "\n",
    "            # Calculate loss for predicting masked tokens\n",
    "            # Flatten both masked_language predictions and bert_labels to match CrossEntropyLoss input requirements\n",
    "            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
    "\n",
    "            # Sum up the two losses\n",
    "            loss = next_loss + mask_loss\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            else:\n",
    "                total_loss += loss.item()\n",
    "                total_next_sentence_loss += next_loss.item()\n",
    "                total_mask_loss += mask_loss.item()\n",
    "                total_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / (total_batches + 1)\n",
    "    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)\n",
    "    avg_mask_loss = total_mask_loss / (total_batches + 1)\n",
    "\n",
    "    print(f\"Average Loss: {avg_loss:.4f}, Average Next Sentence Loss: {avg_next_sentence_loss:.4f}, Average Mask Loss: {avg_mask_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e3621",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa4d8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "# Training loop setup\n",
    "num_epochs = 1\n",
    "total_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "# Define the number of warmup steps, e.g., 10% of total\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
    "#         bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
    "\n",
    "#         next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts)\n",
    "#         mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
    "\n",
    "#         loss = next_loss + mask_loss\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()  # Update the learning rate\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         if torch.isnan(loss):\n",
    "#             continue\n",
    "#         else:\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     avg_train_loss = total_loss / len(train_dataloader) + 1\n",
    "#     train_losses.append(avg_train_loss)\n",
    "#     print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "#     # Evaluation after each epoch\n",
    "#     eval_loss = evaluate(test_dataloader, model, loss_fn_nsp, loss_fn_mlm, device)\n",
    "#     eval_losses.append(eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3386a741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses = [0]\n",
    "eval_losses = [0]\n",
    "# Plotting the loss values\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(range(1,num_epochs+1), train_losses, label=\"Training Loss\", color='blue')\n",
    "plt.scatter(range(1,num_epochs+1), eval_losses, label=\"Evaluation Loss\", color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cdd84",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d03cdda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence follows the first\n",
      "The cat sat on the [UNK].\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer with the BERT model's vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def predict_nsp(sentence1, sentence2, model, tokenizer):\n",
    "    # Tokenize sentences with special tokens\n",
    "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
    "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
    "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        # Assuming the model returns NSP predictions first\n",
    "        nsp_prediction, _ = model(tokens_tensor, segment_tensor)\n",
    "        # Select the first element (first sequence) of the logits tensor\n",
    "        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]\n",
    "        logits = torch.softmax(first_logits, dim=1)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Interpret the prediction\n",
    "    return \"Second sentence follows the first\" if prediction == 1 else \"Second sentence does not follow the first\"\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"The cat sat on the mat.\"\n",
    "sentence2 = \"It was a sunny day\"\n",
    "\n",
    "print(predict_nsp(sentence1, sentence2, model, tokenizer))\n",
    "\n",
    "def predict_mlm(sentence, model, tokenizer):\n",
    "    # Tokenize the input sentence and convert to token IDs, including special tokens\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens_tensor = inputs.input_ids\n",
    "\n",
    "    # Create dummy segment labels filled with zeros, assuming it's needed by your model\n",
    "    segment_labels = torch.zeros_like(tokens_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model, now correctly handling the output tuple\n",
    "        output_tuple = model(tokens_tensor, segment_labels)\n",
    "\n",
    "        # Assuming the second element of the tuple contains the MLM logits\n",
    "        predictions = output_tuple[1]  # Adjusted based on your model's output\n",
    "\n",
    "        # Identify the position of the [MASK] token\n",
    "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Get the predicted index for the [MASK] token from the MLM logits\n",
    "        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
    "\n",
    "        # Replace [MASK] in the original sentence with the predicted token\n",
    "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)\n",
    "\n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat sat on the [MASK].\"\n",
    "print(predict_mlm(sentence, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69f492",
   "metadata": {},
   "source": [
    "# 2c) Full Transformer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5c668",
   "metadata": {},
   "source": [
    "[link to the notebook](https://www.coursera.org/learn/generative-ai-language-modeling-with-transformers/ungradedLti/RI07m/lab-transformers-for-translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00d1ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import multi30k, Multi30k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6f713",
   "metadata": {},
   "source": [
    "# 3) Loading Models and Inference with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39887e0",
   "metadata": {},
   "source": [
    "`tokenizer(text, return_tensors = \"pt\")` the 'pt' means PyTorch, but we can also use 'tf' fot TensorFlow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1b369",
   "metadata": {},
   "source": [
    "## Without Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b23135",
   "metadata": {},
   "source": [
    "### Sentiment analysis with DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb6e05",
   "metadata": {},
   "source": [
    "First, let's initialize a tokenizer and a model for sentiment analysis using DistilBERT fine-tuned on the SST-2 dataset. This setup is useful for tasks where you need to quickly classify the sentiment of a piece of text with a pretrained, efficient transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f73d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\").to(\"cpu\")\n",
    "\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f0cc41",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "083ce2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[  101, 23156,   999,  2017,  1005,  2310,  2180,  1037,  2489,  7281,  2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} \n",
      "\n",
      "output: SequenceClassifierOutput(loss=None, logits=tensor([[-3.9954,  4.3336]]), hidden_states=None, attentions=None) \n",
      "\n",
      "output logits: tensor([[-3.9954,  4.3336]]) \n",
      "\n",
      "output probabilities: tensor([[2.4134e-04, 9.9976e-01]]) \n",
      "\n",
      "predicted class: tensor([1]) which is POSITIVE\n"
     ]
    }
   ],
   "source": [
    "text = \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors = \"pt\") #dict {'input_ids': , 'attention_mask'}\n",
    "print('inputs:',inputs,'\\n') \n",
    "\n",
    "#inference part\n",
    "with torch.no_grad():\n",
    "    output_of_model = model(input_ids = inputs['input_ids'], \n",
    "                            attention_mask = inputs['attention_mask'])\n",
    "print('output:',output_of_model,'\\n')\n",
    "\n",
    "#Post-process the output\n",
    "logits_of_output = output_of_model.logits\n",
    "print('output logits:',logits_of_output,'\\n')\n",
    "\n",
    "probs = torch.softmax(logits_of_output, dim = 1)\n",
    "print('output probabilities:', probs,'\\n')\n",
    "\n",
    "predicted_class = torch.argmax(probs, dim = 1)\n",
    "print('predicted class:', predicted_class,'which is', labels[predicted_class] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdfb90d",
   "metadata": {},
   "source": [
    "### Text generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac804fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9868efa",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39a49b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[ 7454,  2402,   257,   640,    11,   262,   995,   373,  5901,   351,   661,   508,   547,   407,   691,  5527,   475,   635,  3665,    13,   198,   464,   717,  1517,   326,  1625,   284,  2000,   618,   314,  1807,   286,   428,   318,   703,   881,  1637,   484,   550,   287,   511, 16511,   290,   644,  1611,   340,   561,   307,   611,  2130]])\n",
      "Once upon a time, the world was filled with people who were not only rich but also powerful.\n",
      "The first thing that came to mind when I thought of this is how much money they had in their pockets and what kind it would be if someone\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output_ids = model.generate(\n",
    "            input_ids = inputs['input_ids'], \n",
    "            attention_mask = inputs['attention_mask'],\n",
    "            pad_token_id = tokenizer.eos_token_id,\n",
    "            max_length = 50, \n",
    "            num_return_sequences = 1,\n",
    "            temperature = 1,\n",
    "            top_k = 50,\n",
    "            top_p = 95,\n",
    "            repetition_penalty = 1.2\n",
    ")\n",
    "\n",
    "print('outputs:',output_ids)\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc0c42",
   "metadata": {},
   "source": [
    "### Filling Mask with bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c9dee41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: capital, Confidence: 0.86\n",
      "Predicted token: heart, Confidence: 0.07\n",
      "Predicted token: name, Confidence: 0.01\n",
      "Predicted token: king, Confidence: 0.01\n",
      "Predicted token: center, Confidence: 0.01\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"The [MASK] of France is Paris!\"\n",
    "\n",
    "mask_filler = pipeline(task = 'fill-mask',\n",
    "                       model = model, \n",
    "                       tokenizer = tokenizer)\n",
    "\n",
    "results = mask_filler(text)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14439077",
   "metadata": {},
   "source": [
    "## How to use the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92740e4a",
   "metadata": {},
   "source": [
    " <span style=\"background-color: yellow\"> **Pipeline code** </span>\n",
    "```python\n",
    "transformers.pipeline(\n",
    "            task: str,\n",
    "            model: Optional = None,\n",
    "            config: Optional = None,\n",
    "            tokenizer: Optional = None,\n",
    "            feature_extractor: Optional = None,\n",
    "            framework: Optional = None,\n",
    "            revision: str = 'main',\n",
    "            use_fast: bool = True,\n",
    "            model_kwargs: Dict[str, Any] = None,\n",
    "            **kwargs\n",
    ")\n",
    "```\n",
    " <span style=\"background-color: yellow\"> **Parameters** </span>\n",
    "- **task**: `str`\n",
    "  - The task to perform, such as \"text-classification\", \"text-generation\", \"question-answering\", etc.\n",
    "  - Example: `\"text-classification\"`\n",
    "\n",
    "- **model**: `Optional`\n",
    "  - The model to use. This can be a string (model identifier from Hugging Face model hub), a path to a directory containing model files, or a pre-loaded model instance.\n",
    "  - Example: `\"distilbert-base-uncased-finetuned-sst-2-english\"`\n",
    "\n",
    "- **config**: `Optional`\n",
    "  - The configuration to use. This can be a string, a path to a directory, or a pre-loaded config object.\n",
    "  - Example: `{\"output_attentions\": True}`\n",
    "\n",
    "- **tokenizer**: `Optional`\n",
    "  - The tokenizer to use. This can be a string, a path to a directory, or a pre-loaded tokenizer instance.\n",
    "  - Example: `\"bert-base-uncased\"`\n",
    "\n",
    "- **feature_extractor**: `Optional`\n",
    "  - The feature extractor to use for tasks that require it (e.g., image processing).\n",
    "  - Example: `\"facebook/detectron2\"`\n",
    "\n",
    "- **framework**: `Optional`\n",
    "  - The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. If not specified, it will be inferred.\n",
    "  - Example: `\"pt\"`\n",
    "\n",
    "- **revision**: `str`, default `'main'`\n",
    "  - The specific model version to use (branch, tag, or commit hash).\n",
    "  - Example: `\"v1.0\"`\n",
    "\n",
    "- **use_fast**: `bool`, default `True`\n",
    "  - Whether to use the fast version of the tokenizer if available.\n",
    "  - Example: `True`\n",
    "\n",
    "- **model_kwargs**: `Dict[str, Any]`, default `None`\n",
    "  - Additional keyword arguments passed to the model during initialization.\n",
    "  - Example: `{\"output_hidden_states\": True}`\n",
    "\n",
    "- **kwargs**: `Any`\n",
    "  - Additional keyword arguments passed to the pipeline components.\n",
    "<span style=\"background-color: yellow\"> **Task types** </span>\n",
    "The `pipeline()` function supports a wide range of NLP tasks. Here are some of the common tasks:\n",
    "\n",
    "1. **Text Classification**: `text-classification`\n",
    "   - **Purpose**: Classify text into predefined categories.\n",
    "   - **Use Cases**: Sentiment analysis, spam detection, topic classification.\n",
    "\n",
    "2. **Text Generation**: `text-generation`\n",
    "   - **Purpose**: Generate coherent text based on a given prompt.\n",
    "   - **Use Cases**: Creative writing, dialogue generation, story completion.\n",
    "\n",
    "3. **Question Answering**: `question-answering`\n",
    "   - **Purpose**: Answer questions based on a given context.\n",
    "   - **Use Cases**: Building Q&A systems, information retrieval from documents.\n",
    "\n",
    "4. **Named Entity Recognition (NER)**: `ner` (or `token-classification`)\n",
    "   - **Purpose**: Identify and classify named entities (like people, organizations, locations) in text.\n",
    "   - **Use Cases**: Extracting structured information from unstructured text.\n",
    "\n",
    "5. **Summarization**: `summarization`\n",
    "   - **Purpose**: Summarize long pieces of text into shorter, coherent summaries.\n",
    "   - **Use Cases**: Document summarization, news summarization.\n",
    "\n",
    "6. **Translation**: `translation_xx_to_yy` (e.g., `translation_en_to_fr`)\n",
    "   - **Purpose**: Translate text from one language to another.\n",
    "   - **Use Cases**: Language translation, multilingual applications.\n",
    "\n",
    "7. **Fill-Mask**: `fill-mask`\n",
    "   - **Purpose**: Predict masked words in a sentence (useful for masked language modeling).\n",
    "   - **Use Cases**: Language modeling tasks, understanding model predictions.\n",
    "\n",
    "8. **Zero-Shot Classification**: `zero-shot-classification`\n",
    "   - **Purpose**: Classify text into categories without needing training data for those categories.\n",
    "   - **Use Cases**: Flexible and adaptable classification tasks.\n",
    "\n",
    "9. **Feature Extraction**: `feature-extraction`\n",
    "   - **Purpose**: Extract hidden state features from text.\n",
    "   - **Use Cases**: Downstream tasks requiring text representations, such as clustering, similarity, or further custom model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f4090",
   "metadata": {},
   "source": [
    "## With Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc00b7",
   "metadata": {},
   "source": [
    "### Sentiment analysis with DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1710bf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997586607933044}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    task = \"text-classification\",\n",
    "    model = \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    framework =\"pt\"\n",
    ")\n",
    "\n",
    "# Classify a sample text\n",
    "result = classifier(\"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccddf059",
   "metadata": {},
   "source": [
    "### Text generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3213d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, a single child, or child-sized group of children, must be in their care during the daily process of caring for each other, or have at least one child who is not currently in or near their care and at least\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    task = \"text-generation\", \n",
    "    model = \"gpt2\",\n",
    "    framework =\"pt\"\n",
    ")\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "result = generator(prompt, \n",
    "                   max_length = 50,\n",
    "                   num_return_sequences = 1)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74338d5a",
   "metadata": {},
   "source": [
    "### Filling mask with bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b5124ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: capital, Confidence: 0.86\n",
      "Predicted token: heart, Confidence: 0.07\n",
      "Predicted token: name, Confidence: 0.01\n",
      "Predicted token: king, Confidence: 0.01\n",
      "Predicted token: center, Confidence: 0.01\n"
     ]
    }
   ],
   "source": [
    "fill_mask = pipeline(\n",
    "    task = \"fill-mask\", \n",
    "    model = \"bert-base-uncased\",\n",
    "    framework = \"pt\"\n",
    ")\n",
    "\n",
    "prompt = \"The [MASK] of France is Paris!\"\n",
    "result = fill_mask(prompt)\n",
    "\n",
    "# Print the generated text\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20668be",
   "metadata": {},
   "source": [
    "Same but with the tokenizer explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d32189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: capital, Confidence: 0.86\n",
      "Predicted token: heart, Confidence: 0.07\n",
      "Predicted token: name, Confidence: 0.01\n",
      "Predicted token: king, Confidence: 0.01\n",
      "Predicted token: center, Confidence: 0.01\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "pretrained_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the input text with a masked token\n",
    "text = \"The [MASK] of France is Paris!\"\n",
    "\n",
    "# Create the pipeline\n",
    "mask_filler = pipeline(task='fill-mask', model=pretrained_model,tokenizer=pretrained_tokenizer)\n",
    "\n",
    "# Perform inference using the pipeline\n",
    "results = mask_filler(text)\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be22f42f",
   "metadata": {},
   "source": [
    "### Language detection with papluca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "291f5ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'fr', 'score': 0.9934879541397095}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    task = \"text-classification\", \n",
    "    model = \"papluca/xlm-roberta-base-language-detection\",\n",
    "    framework = \"pt\"\n",
    ")\n",
    "result = classifier(\"Bonjour, comment ça va?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6990256",
   "metadata": {},
   "source": [
    "### Language translation with t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e87b689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment êtes-vous?\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    task = \"text2text-generation\", \n",
    "    model = \"t5-small\",\n",
    "    framework = \"pt\"\n",
    ")\n",
    "\n",
    "prompt = \"Translate English to French: How are you?\"\n",
    "result = generator(prompt, max_length = 50, num_return_sequences = 1)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5264b",
   "metadata": {},
   "source": [
    "# 4) MLM Pre-training of a BERT model, with Hugging-Face\n",
    "\n",
    "The main differences with 2b) are:\n",
    "1. The tokenizer and the model are downloaded and not built manually;\n",
    "2. We will not define a dataloader as usual because the `Trainer` class of Hugging-Face already does it internally;\n",
    "3. Importantly, we do not have to do manually the MLM preparation of the dataset because `DataCollatorForLanguageModeling` does it automatically;\n",
    "4. However, there is not an automatic Hugging Face method for doing the NSP task. For this reason, here we do only the MLM task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14544654",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b465839",
   "metadata": {},
   "source": [
    "The WikiText dataset is a widely used benchmark dataset in the field of natural language processing (NLP). The dataset contains a large amount of text extracted from Wikipedia, which is a vast online encyclopedia covering a wide range of topics. The articles in the WikiText dataset are preprocessed to remove formatting, hyperlinks, and other metadata, resulting in a clean text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32da3edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "dataset[\"train\"][36717]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c6c45e",
   "metadata": {},
   "source": [
    "Reduce the datasets to fewer instances to run on CPU, and export datasets to be used in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "129e8e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)])\n",
    "dataset[\"test\"] = dataset[\"test\"].select([i for i in range(200)])\n",
    "\n",
    "output_file_train = \"wikitext_dataset_train.txt\"\n",
    "output_file_test = \"wikitext_dataset_test.txt\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_train, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Iterate over each example in the dataset\n",
    "    for example in dataset[\"train\"]:\n",
    "        # Write the example text to the file\n",
    "        f.write(example[\"text\"] + \"\\n\")\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_test, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Iterate over each example in the dataset\n",
    "    for example in dataset[\"test\"]:\n",
    "        # Write the example text to the file\n",
    "        f.write(example[\"text\"] + \"\\n\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3017f",
   "metadata": {},
   "source": [
    "## Tokenizer, tokenization of the datasets and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8670b4",
   "metadata": {},
   "source": [
    "We define a tokenizer and then train it on our reduced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb7489f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd97673b7f59499da0df5533a50be1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def batch_iterator(batch_size = 10000):\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        yield dataset['train'][i : i + batch_size][\"text\"]\n",
    "\n",
    "bert_tokenizer = bert_tokenizer.train_new_from_iterator(text_iterator = batch_iterator(), vocab_size = len(bert_tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ab58d",
   "metadata": {},
   "source": [
    "The tokenier is ready to tokenize the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6c25e709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f4cc845d05467cb4a36ae7c35a63b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd7a889a33a406cb406556008d8bdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf9ac72a4254abf804b9600e61c2de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 216, 227, 1134, 6262, 799, 733, 15, 619, 799, 1414, 256, 35, 5440, 1124, 32, 16, 32, 1843, 398, 764, 1736, 1373, 1055, 184, 35, 1062, 2489, 188, 1373, 446, 180, 2342, 891, 5343, 1643, 17, 6744, 290, 4689, 636, 8703, 1126, 32, 16, 32, 770, 5333, 227, 9920, 797, 6436, 15, 227, 1348, 4068, 4856, 636, 11604, 7174, 12362, 188, 4856, 636, 8854, 5085, 17, 171, 1121, 6874, 636, 35, 1220, 184, 6670, 2342, 15, 6973, 5114, 216, 2940, 231, 508, 212, 5258, 9167, 636, 188, 10282, 216, 391, 290, 5114, 17, 171, 2375, 191, 769, 1504, 4963, 219, 171, 2939, 6531, 5935, 219, 242, 3626, 1121, 11, 53, 3162, 29, 538, 389, 5627, 256, 3592, 15, 171, 369, 256, 8653, 453, 191, 171, 1121, 17, 1540, 2342, 15, 171, 1121, 1348, 1104, 180, 35, 2393, 15, 764, 2490, 508, 212, 11062, 188, 797, 2018, 6840, 17, 2542, 171, 771, 1504, 2342, 290, 797, 32, 16, 32, 1522, 910, 2342, 3465, 191, 1309, 1469, 1462, 17, 422, 171, 398, 11, 53, 11754, 15, 2829, 7111, 290, 5114, 15, 476, 184, 438, 2412, 35, 4297, 3164, 568, 1380, 1015, 180, 171, 1104, 184, 171, 398, 17, 499, 290, 430, 2632, 9859, 1673, 3066, 191, 171, 398, 11, 53, 425, 771, 8906, 15, 699, 391, 1373, 35, 2662, 2130, 1124, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return bert_tokenizer(examples[\"text\"], truncation = True, padding = \"max_length\", max_length = 512) #add padding up to max_length\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched = True, remove_columns = [\"text\"])\n",
    "\n",
    "# Print tokenized dataset sample\n",
    "print(tokenized_datasets[\"train\"][9]) #this are the indices of the tokens\n",
    "\n",
    "# Split into training and test sets\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231de9b8",
   "metadata": {},
   "source": [
    "Now that our tokenizer is defined and trained, we define a model for pre-training with its configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "720a7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size = len(bert_tokenizer.get_vocab()),  # Specify the vocabulary size(Make sure this number equals the vocab_size of the tokenizer)\n",
    "    hidden_size = 768,  # Set the hidden size\n",
    "    num_hidden_layers = 12,  # Set the number of layers\n",
    "    num_attention_heads = 12,  # Set the number of attention heads\n",
    "    intermediate_size = 3072,  # Set the intermediate size\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0211c0ce",
   "metadata": {},
   "source": [
    "## Collate function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3668250",
   "metadata": {},
   "source": [
    "This line of code sets up a DataCollatorForLanguageModeling from the Hugging Face Transformers library. A data collator is used during training to dynamically create batches of data. For language modeling, particularly for models like BERT that use masked language modeling (MLM), this collator prepares training batches by automatically masking tokens according to a specified probability. Here are the details of the parameters used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "349667d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = bert_tokenizer, \n",
    "    mlm = True, \n",
    "    mlm_probability = 0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefc76b",
   "metadata": {},
   "source": [
    "## Pre-training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cb86b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./trained_model\",  # Specify the output directory for the trained model\n",
    "    overwrite_output_dir = True, # If set to True, this will overwrite the contents of the output directory if it already exists\n",
    "    do_eval = True, # If True, the model will be evaluated at the specified intervals.\n",
    "    evaluation_strategy = \"epoch\", # the model will be evaluated at the end of each epoch.\n",
    "    learning_rate = 5e-5,\n",
    "    num_train_epochs = 10,  # Specify the number of training epochs\n",
    "    per_device_train_batch_size = 2,  # Set the batch size for training\n",
    "    save_total_limit = 2,  # Limit the total number of saved checkpoints\n",
    "    logging_steps = 20\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1837818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1aa3dc",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4214719",
   "metadata": {},
   "source": [
    "download the weights of the pre-trained model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05b344",
   "metadata": {},
   "source": [
    "It takes 10 min:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85e8d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BeXRxFT2EyQAmBHvxVaMYQ/bert-scratch-model.pt'\n",
    "# model.resize_token_embeddings(30522)\n",
    "# model.load_state_dict(torch.load('bert-scratch-model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e283ac",
   "metadata": {},
   "source": [
    "Now see the very low performances (make comparison with the pre-trained BERT above in 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9c3df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: suri, Confidence: 0.00\n",
      "Predicted token: spells, Confidence: 0.00\n",
      "Predicted token: 158, Confidence: 0.00\n",
      "Predicted token: blurring, Confidence: 0.00\n",
      "Predicted token: signed, Confidence: 0.00\n"
     ]
    }
   ],
   "source": [
    "text = \"The [MASK] of France is Paris.\"\n",
    "\n",
    "# Create a pipeline for the \"fill-mask\" task\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", \n",
    "    model = model, \n",
    "    tokenizer = bert_tokenizer,\n",
    "    device = 'cpu'  # -1 indica CPU\n",
    ")\n",
    "\n",
    "# Generate predictions by filling the mask in the input text\n",
    "results = mask_filler(text) #top_k parameter can be set \n",
    "\n",
    "# Print the predicted sequences\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Trans_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
