{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e987dba",
   "metadata": {},
   "source": [
    "##### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "593c0a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/gw8dmgsd6m11bg8nhrgpd3vr0000gn/T/ipykernel_71656/1304717675.py:15: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources            # package and dependency management\n"
     ]
    }
   ],
   "source": [
    "########################## UTILITY AND SYSTEM ##########################\n",
    "\n",
    "import os                       # filesystem operations\n",
    "import csv                      # reading/writing CSV files\n",
    "import json                     # JSON parsing and serialization\n",
    "import math                     # basic math functions\n",
    "import random                   # random number generation\n",
    "import time                     # time-related functions\n",
    "import tempfile                 # temporary file management\n",
    "import tarfile                  # tar archive handling\n",
    "import io                       # input/output streams\n",
    "import pickle                   # object serialization\n",
    "import importlib                # dynamic import of modules\n",
    "import multiprocessing          # parallel process management\n",
    "import pkg_resources            # package and dependency management\n",
    "from copy import deepcopy       # deep copy of objects\n",
    "from pathlib import Path        # filesystem paths handling (cross-platform)\n",
    "\n",
    "########################## DOWNLOAD ##########################\n",
    "\n",
    "import requests                 # HTTP requests library\n",
    "import wget                     # file downloads from URLs\n",
    "from urllib.request import urlopen  # open URLs (alternative to requests)\n",
    "\n",
    "########################## VISUALIZATION ##########################\n",
    "\n",
    "import matplotlib.pyplot as plt # basic plotting library\n",
    "import plotly.graph_objs as go  # interactive plotting\n",
    "from tqdm.notebook import tqdm  # progress bars for loops in notebooks\n",
    "from pprint import pprint       # formatted pretty-printing of objects\n",
    "\n",
    "########################## DATAFRAME ##########################\n",
    "\n",
    "import numpy as np              # numerical arrays and operations\n",
    "import pandas as pd             # dataframes and data manipulation\n",
    "\n",
    "########################## TEXT PROCESSING ##########################\n",
    "\n",
    "import re                      # regular expressions\n",
    "import string                  # string constants and operations\n",
    "from itertools import chain, islice  # advanced iteration and chaining\n",
    "\n",
    "########################## TOKENIZATION ##########################\n",
    "\n",
    "from collections import Counter, OrderedDict  # frequency counts and ordered dictionaries\n",
    "import nltk                                   # natural language processing toolkit\n",
    "from nltk.tokenize import word_tokenize       # word tokenization\n",
    "import spacy                                  # advanced NLP (tokenization, parsing)\n",
    "from torchtext.data.utils import get_tokenizer       # torchtext tokenizers\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator # build vocabulary from iterator\n",
    "\n",
    "########################## DATASET AND DATALOADER ##########################\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split   # datasets and data loading utilities\n",
    "from torch.nn.utils.rnn import pad_sequence                      # padding variable-length sequences\n",
    "from datasets import load_dataset, DatasetDict                   # HuggingFace datasets loading\n",
    "from torchtext.datasets import AG_NEWS                           # torchtext built-in datasets\n",
    "\n",
    "########################## PYTORCH AND DEEP LEARNING ##########################\n",
    "\n",
    "import torch                             # PyTorch main library\n",
    "from torch import nn, Tensor             # neural network modules and tensors\n",
    "from torch.nn import CrossEntropyLoss    # common loss function for classification\n",
    "\n",
    "########################## WORD EMBEDDING ##########################\n",
    "\n",
    "from torchtext.vocab import GloVe        # pretrained GloVe embeddings\n",
    "# from gensim.models import Word2Vec     # word2vec embeddings from corpus (commented out)\n",
    "\n",
    "########################## HUGGING FACE ##########################\n",
    "\n",
    "import transformers                      # transformers library core\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,     # GPT-2 tokenizer and model\n",
    "    BertTokenizer, BertTokenizerFast, BertConfig, BertForMaskedLM,  # BERT components\n",
    "    XLNetTokenizer,                     # XLNet tokenizer\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline,                          # easy pipelines for inference\n",
    "    AutoTokenizer,                    # auto tokenizer loader\n",
    "    AutoModelForCausalLM, GPT2ForSequenceClassification,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer,  # training utilities\n",
    "    set_seed, GenerationConfig,\n",
    "    BertModel                        # BERT base model\n",
    ")\n",
    "from datasets import DatasetDict         # HuggingFace dataset dictionaries\n",
    "\n",
    "######################### TRL & PEFT (TRAINING & PARAMETER EFFICIENT FINE-TUNING) ##########################\n",
    "\n",
    "from trl import (\n",
    "    SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM,\n",
    "    DPOConfig, DPOTrainer,\n",
    "    RewardTrainer, RewardConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torchmetrics import Accuracy        # metrics for evaluation\n",
    "\n",
    "########################## RAG ##########################\n",
    "\n",
    "from transformers import (\n",
    "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n",
    "    DPRContextEncoder, DPRContextEncoderTokenizer\n",
    ")\n",
    "import faiss                              # similarity search library\n",
    "\n",
    "########################## EVALUATION ##########################\n",
    "\n",
    "import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154853d",
   "metadata": {},
   "source": [
    "# 0) Observation on yeld_tokens function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58e95b",
   "metadata": {},
   "source": [
    "```\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenized_dataset = map(lambda x: tokenizer(x[1]), dataset)  # dataset[i][1] is the text\n",
    "vocab = build_vocab_from_iterator(tokenized_dataset, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) \n",
    "```\n",
    "\n",
    "and \n",
    "```\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "```\n",
    "are equivalent. If we have the dataset already in the memory (es. dataset = [(1,\"Introduction to NLP\"),(2,\"Basics of PyTorch\")] ) we can use both, but if we are extracting a dataset from torchtext for example, **we must use yeld_tokens**, because the dataset itself is an iterator and we can not obtain the sentences as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8c6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter): #this build an iterator\n",
    "    for index,text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbb059",
   "metadata": {},
   "source": [
    "# 1) Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27bf99",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%202.png\" width=\"50%\" alt=\"Image Description\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795ceb3",
   "metadata": {},
   "source": [
    "## Word-based (spacy, nltk)\n",
    "Preserves semantic meaning, increases overall vocabulary; it assigns different IDs to 'unicorn' and 'unicorns'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58265cff",
   "metadata": {},
   "source": [
    "Import spacy models if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95aab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# DO_INSTALL = 0\n",
    "\n",
    "# if DO_INSTALL:\n",
    "#     subprocess.check_call([\n",
    "#         \"python\", \"-m\", \"spacy\", \"download\", \"de_core_news_sm\"\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c917c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (spacy): ['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.', 'I', 'could', \"n't\", 'see', 'it', 'today']\n",
      "Unicorns NOUN nsubj\n",
      "Tokens (nltk ): ['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.', 'I', 'could', \"n't\", 'see', 'it', 'today']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/alex/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "token_list = [token.text for token in doc]\n",
    "print(f\"Tokens (spacy): {token_list}\")\n",
    "print(doc[0].text, doc[0].pos_, doc[0].dep_) #details of a token\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "token_list = word_tokenize(text)\n",
    "print(f\"Tokens (nltk ): {token_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81d582",
   "metadata": {},
   "source": [
    "## Character-based\n",
    "Small vocabulary but without semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f037e8",
   "metadata": {},
   "source": [
    "## Subword-based (WordPiece, Unigram, SentencePiece)\n",
    "Frequently used words stay unplit, while breaking down infrequent words. 'Unicorns' becomes 'unicorn' and 's'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466e915",
   "metadata": {},
   "source": [
    "The ## means that they **are not** new words; the _ means that they **are** new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74df3535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (bert): ['unicorn', '##s', 'are', 'real', '.', 'i', 'saw', 'a', 'unicorn', 'yesterday', '.', 'i', 'couldn', \"'\", 't', 'see', 'it', 'today']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175f5a852c834598b8b3acf768f86d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4317405f6f5b4450932133411b40cb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df25a14352af4575aa262f562c5931e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (xlnet): ['▁Uni', 'corn', 's', '▁are', '▁real', '.', '▁I', '▁saw', '▁a', '▁', 'uni', 'corn', '▁yesterday', '.', '▁I', '▁couldn', \"'\", 't', '▁see', '▁it', '▁today']\n"
     ]
    }
   ],
   "source": [
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # the WordPiece tokenizer is implemented in BertTokenizer\n",
    "token_list = tokenizer.tokenize(text)\n",
    "print(f\"Tokens (bert): {token_list}\")\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\") # Unigram and SentencePiece \n",
    "token_list = tokenizer.tokenize(text)\n",
    "print(f\"Tokens (xlnet): {token_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79795989",
   "metadata": {},
   "source": [
    "## Using PyTorch (Words and sub-words)\n",
    "\n",
    "In PyTorch, especially with the torchtext library, the tokenizer breaks down text from a data set into individual words or subwords, facilitating their conversion into numerical format. After tokenization, the vocab (vocabulary) maps these tokens to unique integers, allowing them to be fed into neural networks. This process is vital because deep learning models operate on numerical data and cannot process raw text directly. Thus, tokenization and vocabulary mapping serve as a bridge between human-readable text and machine-operable numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525ec16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (PyTorch): ['introduction', 'to', 'nlp']\n",
      "['introduction', 'to', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "dataset = [ #it is an iterable, like a dataloader (think to the future application in DL)\n",
    "    (1,\"Introduction to NLP\"), #the first entry is a sentiment label\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\")]\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english') # TorchText tokenizer\n",
    "token_list = tokenizer(dataset[0][1])\n",
    "print(f\"Tokens (PyTorch): {token_list}\")\n",
    "\n",
    "def yield_tokens(data_iter): #this build an iterator\n",
    "    for sentiment, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "my_iterator = yield_tokens(dataset) \n",
    "print(next(my_iterator))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32146679",
   "metadata": {},
   "source": [
    "Then we build a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c594e85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary: 15 \n",
      "\n",
      "Vocabulary: {'with': 14, 'to': 13, 'of': 9, 'nlp': 1, 'classification': 4, 'named': 8, 'for': 6, 'text': 12, 'entity': 5, 'techniques': 11, '<unk>': 0, 'basics': 3, 'recognition': 10, 'pytorch': 2, 'introduction': 7} \n",
      "\n",
      "14\n",
      "with\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials = ['<unk>']) #assigns numbers to token, and '<unk>' to words out of vocabulary\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "print('Number of words in the vocabulary:',len(vocab), '\\n')\n",
    "vocab.get_itos() #**index_to_string** this is a list of the words in the vocabulary, already sorted by indexing (see the last two lines here)\n",
    "print('Vocabulary:',vocab.get_stoi(),'\\n') #this is a dictionary {'word in vocabulary': integer ID}\n",
    "\n",
    "print(vocab.get_stoi()['with'])\n",
    "print(vocab.get_itos()[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefa3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['introduction', 'to', 'nlp']\n",
      "Token Indices: [7, 13, 1]\n",
      "\n",
      "Tokenized Sentence: ['basics', 'of', 'pytorch']\n",
      "Token Indices: [3, 9, 2]\n",
      "\n",
      "Tokenized Sentence: ['nlp', 'techniques', 'for', 'text', 'classification']\n",
      "Token Indices: [1, 11, 6, 12, 4]\n",
      "\n",
      "Tokenized Sentence: ['named', 'entity', 'recognition', 'with', 'pytorch']\n",
      "Token Indices: [8, 5, 10, 14, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_iterator = yield_tokens(dataset) \n",
    "\n",
    "def get_tokenized_sentences_and_IDs(iterator, vocab):\n",
    "    for tokenized_sentence in iterator:\n",
    "        token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "        print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "        print(\"Token Indices:\", token_indices)\n",
    "        print()  # riga vuota per separare\n",
    "\n",
    "get_tokenized_sentences_and_IDs(my_iterator,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42c63576",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey’s been enlightening and, honestly, a bunch of fun.\n",
    "Eager to see where this learning path takes me next!\"\n",
    "\"\"\"\n",
    "\n",
    "# Counting and displaying tokens and their frequency\n",
    "\n",
    "def show_frequencies(tokens, method_name):\n",
    "    print(f\"{method_name} Token Frequencies: {dict(Counter(tokens))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a0ecf",
   "metadata": {},
   "source": [
    "# 2) Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c550e4",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794dbbd",
   "metadata": {},
   "source": [
    "In the Dataset class, the methods len e getitem are not defined because they depend strictly on the type on objects we have in the databse. So it is common practice to define a CustomDataset class, with the methods len e getitem!\n",
    "\n",
    "DataLoader is an iterable but not an iterator:\n",
    "1. data_iterator = iter(dataloader)\n",
    "2. first_batch = next(data_iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a3d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You are awesome!', \"Fame's a fickle friend, Harry.\"]\n",
      "['Soon we must all face the choice between what is right and what is easy.', 'It is our choices, Harry, that show what we truly are, far more than our abilities.']\n",
      "['Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.', \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer = None, vocabulary = None):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "    \n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "len(custom_dataset) ==  custom_dataset.__len__() #number of sentences, method __len__\n",
    "# custom_dataset[i] == custom_dataset.__getitem__(i) #is the i-th sentence, method __getitem\n",
    "\n",
    "dataloader = DataLoader(dataset = custom_dataset, batch_size = 2, shuffle = True)\n",
    "len(dataloader) # number of batches, of dimension 'batch_size'\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb8ac9",
   "metadata": {},
   "source": [
    "## For NLP purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4824661",
   "metadata": {},
   "source": [
    "### Modifying the dataset (not good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99cfc2",
   "metadata": {},
   "source": [
    "For a NLP task, we initialize the class by passing the sentences, a tokenizer, and a vocabulary. Problem is thata dataloader expects all data to have the same lenght, so we have to add padding manually, ptherwise we eill encounter an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef72f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "         21,  1],\n",
      "        [19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0]])\n",
      "batch shape: torch.Size([2, 20])\n",
      "['it', 'is', 'our', 'choices', ',', 'harry', ',', 'that', 'show', 'what', 'we', 'truly', 'are', ',', 'far', 'more', 'than', 'our', 'abilities', '.']\n",
      "['you', 'are', 'awesome', '!', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n",
      "tensor([[54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "          2, 12, 64, 17, 26, 65,  1]])\n",
      "batch shape: torch.Size([2, 25])\n",
      "['soon', 'we', 'must', 'all', 'face', 'the', 'choice', 'between', 'what', 'is', 'right', 'and', 'what', 'is', 'easy', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['youth', 'can', 'not', 'know', 'how', 'age', 'thinks', 'and', 'feels', '.', 'but', 'old', 'men', 'are', 'guilty', 'if', 'they', 'forget', 'what', 'it', 'was', 'to', 'be', 'young', '.']\n",
      "\n",
      "tensor([[35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "         43, 61,  9, 44,  0, 14,  9, 33,  1]])\n",
      "batch shape: torch.Size([2, 27])\n",
      "['fame', \"'\", 's', 'a', 'fickle', 'friend', ',', 'harry', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['if', 'you', 'want', 'to', 'know', 'what', 'a', 'man', \"'\", 's', 'like', ',', 'take', 'a', 'good', 'look', 'at', 'how', 'he', 'treats', 'his', 'inferiors', ',', 'not', 'his', 'equals', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, vocabulary):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.sentences[idx])\n",
    "        # Convert tokens to tensor indices using vocab\n",
    "        tensor_indices = [self.vocabulary[token] for token in tokens]\n",
    "        return torch.tensor(tensor_indices)\n",
    "    \n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
    "\n",
    "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
    "custom_dataset[5] #IDs of the 5-th sentence\n",
    "\n",
    "# dataloader = DataLoader(custom_dataset, batch_size= 2 , shuffle=True)\n",
    "# for batch in dataloader:\n",
    "#     print(batch)              #this will arise an error\n",
    "\n",
    "def collate_fn(batch):\n",
    "    padded_batch = pad_sequence(batch, batch_first = True, padding_value = 0) # padding_value numerical value of the padding\n",
    "    return padded_batch\n",
    "\n",
    "dataloader = DataLoader(dataset = custom_dataset, batch_size = 2 , shuffle = True, collate_fn = collate_fn)\n",
    "for batch in dataloader:\n",
    "    print(batch) #batch is [batch_size x sequence_lenght] if batch_first = True, otherwise (default) [sequence_lenght x batch_size]\n",
    "    print(f'batch shape: {batch.shape}')\n",
    "    for row in batch:\n",
    "        for idx in row:\n",
    "            words = [vocab.get_itos()[idx] for idx in row]\n",
    "        print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f0307",
   "metadata": {},
   "source": [
    "### Without modifying the dataset: best practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f098f4a",
   "metadata": {},
   "source": [
    "We have the option to utilize the collate function for tasks such as tokenization, converting tokenized indices, and transforming the result into a tensor. It's important to note that the original data set remains untouched by these transformations. In this way we can still access the raw data as custom_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039a74e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35,  6, 16,  3, 38, 40,  0,  8,  1],\n",
      "        [19,  4, 25, 20,  0,  0,  0,  0,  0]])\n",
      "tensor([[12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "         21,  1],\n",
      "        [54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1,  0,  0,\n",
      "          0,  0]])\n",
      "tensor([[11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "         43, 61,  9, 44,  0, 14,  9, 33,  1],\n",
      "        [66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "          2, 12, 64, 17, 26, 65,  1,  0,  0]])\n",
      "\n",
      "tensor([[11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "         43, 61,  9, 44,  0, 14,  9, 33,  1],\n",
      "        [54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "batch shape: torch.Size([2, 27])\n",
      "['if', 'you', 'want', 'to', 'know', 'what', 'a', 'man', \"'\", 's', 'like', ',', 'take', 'a', 'good', 'look', 'at', 'how', 'he', 'treats', 'his', 'inferiors', ',', 'not', 'his', 'equals', '.']\n",
      "['soon', 'we', 'must', 'all', 'face', 'the', 'choice', 'between', 'what', 'is', 'right', 'and', 'what', 'is', 'easy', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n",
      "tensor([[19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "          2, 12, 64, 17, 26, 65,  1]])\n",
      "batch shape: torch.Size([2, 25])\n",
      "['you', 'are', 'awesome', '!', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['youth', 'can', 'not', 'know', 'how', 'age', 'thinks', 'and', 'feels', '.', 'but', 'old', 'men', 'are', 'guilty', 'if', 'they', 'forget', 'what', 'it', 'was', 'to', 'be', 'young', '.']\n",
      "\n",
      "tensor([[12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "         21,  1],\n",
      "        [35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0]])\n",
      "batch shape: torch.Size([2, 20])\n",
      "['it', 'is', 'our', 'choices', ',', 'harry', ',', 'that', 'show', 'what', 'we', 'truly', 'are', ',', 'far', 'more', 'than', 'our', 'abilities', '.']\n",
      "['fame', \"'\", 's', 'a', 'fickle', 'friend', ',', 'harry', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "custom_dataset = CustomDataset(sentences) #still have the raw text!\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    tensor_batch = []\n",
    "    for sentence in batch:\n",
    "        tokens = tokenizer(sentence)\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "    padded_batch = pad_sequence(tensor_batch, batch_first=True)\n",
    "    return padded_batch\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset = custom_dataset,   # Custom PyTorch Dataset containing your data\n",
    "    batch_size = 2,     # Number of samples in each mini-batch\n",
    "    shuffle = True,              # Shuffle the data at the beginning of each epoch\n",
    "    collate_fn = my_collate_fn      # Custom collate function for processing batches\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "\n",
    "print()\n",
    "for batch in dataloader:\n",
    "    print(batch) #batch is [batch_size x sequence_lenght] if batch_first = True, otherwise (default) [sequence_lenght x batch_size]\n",
    "    print(f'batch shape: {batch.shape}')\n",
    "    for row in batch:\n",
    "        for idx in row:\n",
    "            words = [vocab.get_itos()[idx] for idx in row]\n",
    "        print(words)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trans_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
